# 📊 数据预处理模块详解

## 🎯 模块概述

数据预处理模块是抑郁风险预警系统的核心组件，负责将原始的社交媒体文本数据转换为机器学习模型可以理解的标准化特征。整个模块采用**模块化设计**，分为三个核心组件：

```
数据预处理模块
├── 🧹 TextCleaner (文本清洗器)
├── 🔍 FeatureExtractor (特征提取器)  
└── 🎛️ DataProcessor (数据处理器)
```

---

## 🧹 1. 文本清洗器 (TextCleaner)

### 📋 主要功能
将混乱的社交媒体文本转换为干净、标准化的文本数据。

### 🔧 核心逻辑

#### 1.1 初始化阶段
```python
class TextCleaner:
    def __init__(self):
        # 表情符号映射表
        self.emoticon_map = {
            ':)': ' [happy] ', ':-)': ' [happy] ',
            ':(': ' [sad] ', ':-(': ' [sad] ',
            # ... 更多表情符号
        }
        
        # 正则表达式模式
        self.patterns = {
            'url': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'(\+?1?[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})',
            # ... 更多模式
        }
```

**解释**：
- **表情符号映射**：将各种表情符号统一转换为标准标签，便于后续分析
- **正则表达式模式**：定义各种需要处理的文本模式（URL、邮箱、电话等）

#### 1.2 文本清洗流程
```python
def clean_text(self, text: str, remove_urls=True, ...) -> str:
    # 1. HTML解码
    text = html.unescape(text)
    
    # 2. Unicode标准化
    text = unicodedata.normalize('NFKC', text)
    
    # 3. 移除各种模式
    if remove_urls:
        text = self.compiled_patterns['url'].sub(' [URL] ', text)
    
    # 4. 标准化表情符号
    if normalize_emoticons:
        text = self._normalize_emoticons(text)
    
    # 5. 处理重复内容
    if remove_repeated_chars:
        text = self.compiled_patterns['repeated_chars'].sub(r'\1\1', text)
    
    # 6. 转换为小写并清理空格
    if lowercase:
        text = text.lower()
    text = self.compiled_patterns['extra_spaces'].sub(' ', text)
```

**清洗步骤详解**：

1. **HTML解码**：将 `&amp;` 等HTML实体转换为正常字符
2. **Unicode标准化**：统一字符编码格式
3. **模式替换**：将URL、邮箱等替换为标准化标签
4. **表情符号标准化**：将 `:)` 转换为 `[happy]`
5. **重复内容处理**：将 `loooool` 转换为 `lool`
6. **格式标准化**：转换为小写，清理多余空格

#### 1.3 特征提取
```python
def extract_features(self, text: str) -> Dict[str, any]:
    features = {
        'text_length': len(text),
        'word_count': len(text.split()),
        'has_url': bool(self.compiled_patterns['url'].search(text)),
        'has_email': bool(self.compiled_patterns['email'].search(text)),
        'emoticon_count': 0,
        # ... 更多特征
    }
```

**提取的特征**：
- **基本统计**：文本长度、单词数量、字符数量
- **内容检测**：是否包含URL、邮箱、电话等
- **特殊内容**：表情符号数量、重复字符/单词数量

---

## 🔍 2. 特征提取器 (FeatureExtractor)

### 📋 主要功能
从清洗后的文本中提取有意义的特征，为机器学习模型提供输入。

### 🔧 核心逻辑

#### 2.1 抑郁相关词汇库
```python
self.depression_keywords = {
    '情绪低落': ['sad', 'depressed', 'down', 'blue', 'unhappy', 'miserable', 'hopeless'],
    '兴趣丧失': ['uninterested', 'bored', 'no_interest', 'nothing_matters', 'empty'],
    '睡眠问题': ['insomnia', 'sleep', 'tired', 'exhausted', 'fatigue', 'restless'],
    '食欲变化': ['appetite', 'hungry', 'not_hungry', 'weight_loss', 'weight_gain'],
    '注意力问题': ['concentrate', 'focus', 'attention', 'distracted', 'mind_wandering'],
    '自我评价': ['worthless', 'failure', 'useless', 'guilty', 'blame_myself'],
    '自杀想法': ['suicide', 'kill_myself', 'death', 'die', 'end_it_all', 'better_off_dead'],
    '焦虑症状': ['anxious', 'worried', 'nervous', 'panic', 'fear', 'stress'],
    '身体症状': ['pain', 'ache', 'sick', 'ill', 'headache', 'stomach'],
    '社交退缩': ['alone', 'lonely', 'isolated', 'no_friends', 'avoid_people']
}
```

**解释**：基于PHQ-9抑郁筛查问卷，定义了10个抑郁症状类别的关键词。

#### 2.2 语言学特征提取
```python
def extract_linguistic_features(self, text: str) -> Dict[str, float]:
    features = {}
    
    # 基本统计特征
    features['text_length'] = len(text)
    features['word_count'] = len(text.split())
    features['sentence_count'] = len(re.split(r'[.!?]+', text))
    
    # 词汇多样性
    words = text.lower().split()
    if words:
        features['unique_words'] = len(set(words))
        features['lexical_diversity'] = features['unique_words'] / features['word_count']
    
    # 标点符号特征
    features['exclamation_count'] = len(self.compiled_patterns['exclamation'].findall(text))
    features['question_count'] = len(self.compiled_patterns['question'].findall(text))
    features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0
```

**语言学特征包括**：
- **文本统计**：长度、单词数、句子数、平均长度
- **词汇多样性**：独特单词数、词汇多样性比率
- **标点符号**：感叹号、问号、省略号数量
- **格式特征**：大写字母比例、全大写单词数

#### 2.3 抑郁特征提取
```python
def extract_depression_features(self, text: str) -> Dict[str, float]:
    features = {}
    text_lower = text.lower()
    
    # 统计各类抑郁关键词
    for category, keywords in self.depression_keywords.items():
        count = sum(text_lower.count(keyword) for keyword in keywords)
        features[f'{category}_count'] = count
        features[f'{category}_density'] = count / len(text_lower.split()) if text_lower.split() else 0
    
    # 计算总体抑郁指标
    total_depression_words = sum(features[f'{cat}_count'] for cat in self.depression_keywords.keys())
    features['total_depression_count'] = total_depression_words
    features['depression_density'] = total_depression_words / len(text_lower.split()) if text_lower.split() else 0
```

**抑郁特征包括**：
- **分类统计**：每个抑郁症状类别的关键词数量
- **密度指标**：关键词在文本中的密度
- **总体指标**：总抑郁词汇数量和密度

#### 2.4 情感特征提取
```python
def extract_emotion_features(self, text: str) -> Dict[str, float]:
    features = {}
    text_lower = text.lower()
    
    # 统计各类情感词汇
    for emotion, words in self.emotion_words.items():
        count = sum(text_lower.count(word) for word in words)
        features[f'{emotion}_count'] = count
        features[f'{emotion}_density'] = count / len(text_lower.split()) if text_lower.split() else 0
    
    # 计算情感极性
    positive_count = features['positive_count']
    negative_count = features['negative_count']
    total_words = len(text_lower.split())
    
    if total_words > 0:
        features['emotion_polarity'] = (positive_count - negative_count) / total_words
        features['emotion_intensity'] = (positive_count + negative_count) / total_words
    else:
        features['emotion_polarity'] = 0
        features['emotion_intensity'] = 0
```

**情感特征包括**：
- **情感统计**：积极、消极、中性词汇数量
- **情感密度**：各类情感词汇的密度
- **情感极性**：积极-消极词汇的差值
- **情感强度**：积极+消极词汇的总密度

#### 2.5 社交媒体特征提取
```python
def extract_social_media_features(self, text: str) -> Dict[str, float]:
    features = {}
    
    # 社交媒体特定模式
    features['hashtag_count'] = len(re.findall(r'#\w+', text))
    features['mention_count'] = len(re.findall(r'@\w+', text))
    features['url_count'] = len(re.findall(r'http[s]?://\S+', text))
    features['emoticon_count'] = len(re.findall(r'[:;=]-?[)(/|\\pPoO]', text))
    
    # 重复内容
    features['repeated_char_count'] = len(re.findall(r'(.)\1{2,}', text))
    features['repeated_word_count'] = len(re.findall(r'\b(\w+)(\s+\1){2,}\b', text))
    
    # 标点符号密度
    total_words = len(text.split())
    if total_words > 0:
        features['exclamation_density'] = features['exclamation_count'] / total_words
        features['question_density'] = features['question_count'] / total_words
```

**社交媒体特征包括**：
- **平台特征**：话题标签、@提及、URL、表情符号数量
- **重复内容**：重复字符、重复单词数量
- **表达方式**：感叹号、问号密度

---

## 🎛️ 3. 数据处理器 (DataProcessor)

### 📋 主要功能
整合文本清洗和特征提取，处理整个数据集，为机器学习模型准备训练数据。

### 🔧 核心逻辑

#### 3.1 数据加载
```python
def load_data(self, file_path: str, file_type: str = 'auto') -> pd.DataFrame:
    """加载数据文件"""
    if file_type == 'auto':
        file_type = Path(file_path).suffix.lower()
    
    if file_type == '.csv':
        data = pd.read_csv(file_path)
    elif file_type == '.json':
        data = pd.read_json(file_path)
    elif file_type == '.txt':
        data = pd.read_csv(file_path, sep='\t')
    else:
        raise ValueError(f"不支持的文件类型: {file_type}")
    
    return data
```

**支持的文件格式**：
- **CSV文件**：逗号分隔值
- **JSON文件**：JavaScript对象表示法
- **TXT文件**：制表符分隔的文本文件

#### 3.2 数据过滤
```python
def filter_data(self, data: pd.DataFrame) -> pd.DataFrame:
    """过滤数据"""
    # 移除空值
    data = data.dropna(subset=[self.text_column])
    
    # 按文本长度过滤
    data = data[
        (data[self.text_column].str.len() >= self.min_text_length) &
        (data[self.text_column].str.len() <= self.max_text_length)
    ]
    
    return data.reset_index(drop=True)
```

**过滤条件**：
- **空值处理**：移除文本为空的行
- **长度过滤**：只保留指定长度范围内的文本

#### 3.3 特征提取和标准化
```python
def prepare_features(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    """准备特征数据"""
    # 批量提取特征
    feature_dicts = self.extract_batch_features(data[self.text_column].tolist())
    
    # 转换为数组
    feature_array = self.features_to_array(feature_dicts)
    
    # 特征标准化
    if self.scaler is None:
        self.scaler = StandardScaler()
        feature_array = self.scaler.fit_transform(feature_array)
    else:
        feature_array = self.scaler.transform(feature_array)
    
    # 标签编码
    if self.label_encoder is None:
        self.label_encoder = LabelEncoder()
        labels = self.label_encoder.fit_transform(data[self.label_column])
    else:
        labels = self.label_encoder.transform(data[self.label_column])
    
    return feature_array, labels
```

**处理步骤**：
1. **特征提取**：对每个文本提取所有特征
2. **数组转换**：将特征字典转换为数值数组
3. **特征标准化**：使用StandardScaler标准化特征
4. **标签编码**：使用LabelEncoder编码标签

#### 3.4 数据分割
```python
def split_data(self, features: np.ndarray, labels: np.ndarray, 
               test_size: float = 0.2, random_state: int = 42) -> Tuple:
    """分割训练和测试数据"""
    return train_test_split(
        features, labels, 
        test_size=test_size, 
        random_state=random_state,
        stratify=labels  # 保持标签分布
    )
```

**分割策略**：
- **比例分割**：默认80%训练，20%测试
- **分层采样**：保持各类别在训练和测试集中的比例一致

#### 3.5 状态保存和加载
```python
def save_processor(self, file_path: str):
    """保存处理器状态"""
    processor_state = {
        'scaler': self.scaler,
        'label_encoder': self.label_encoder,
        'feature_names': self.feature_names,
        'text_column': self.text_column,
        'label_column': self.label_column,
        'min_text_length': self.min_text_length,
        'max_text_length': self.max_text_length
    }
    
    with open(file_path, 'wb') as f:
        pickle.dump(processor_state, f)

def load_processor(self, file_path: str):
    """加载处理器状态"""
    with open(file_path, 'rb') as f:
        processor_state = pickle.load(f)
    
    # 恢复状态
    for key, value in processor_state.items():
        setattr(self, key, value)
```

**状态管理**：
- **保存状态**：将标准化器、编码器等保存到文件
- **加载状态**：从文件恢复处理器状态，确保一致性

---

## 🔄 完整处理流程

### 📊 数据流程图
```
原始数据 → 数据加载 → 数据过滤 → 文本清洗 → 特征提取 → 特征标准化 → 数据分割 → 训练数据
```

### 💻 使用示例
```python
# 1. 创建数据处理器
processor = DataProcessor(
    text_column='text',
    label_column='label',
    min_text_length=10,
    max_text_length=1000
)

# 2. 加载和预处理数据
data = processor.load_data('data/raw/sample_data.csv')
data = processor.filter_data(data)

# 3. 准备特征
features, labels = processor.prepare_features(data)

# 4. 分割数据
X_train, X_test, y_train, y_test = processor.split_data(features, labels)

# 5. 保存处理器状态
processor.save_processor('models/preprocessor.pkl')
```

---

## 🎯 特征总结

### 📈 提取的特征类型

| 特征类型 | 特征数量 | 说明 |
|---------|---------|------|
| **语言学特征** | 12个 | 文本长度、词汇多样性、标点符号等 |
| **抑郁特征** | 20个 | 10个症状类别 × 2个指标（数量+密度） |
| **情感特征** | 8个 | 积极/消极/中性词汇统计和极性 |
| **社交媒体特征** | 8个 | 话题标签、@提及、重复内容等 |
| **总计** | **48个特征** | 为机器学习模型提供丰富的输入 |

### 🔍 关键特征示例

**抑郁风险指标**：
- `total_depression_count`：总抑郁词汇数量
- `depression_density`：抑郁词汇密度
- `suicide_count`：自杀相关词汇数量

**情感状态指标**：
- `emotion_polarity`：情感极性（正值=积极，负值=消极）
- `emotion_intensity`：情感强度
- `negative_density`：消极词汇密度

**表达方式指标**：
- `exclamation_density`：感叹号密度
- `repeated_char_count`：重复字符数量
- `uppercase_ratio`：大写字母比例

---

## 🚀 模块优势

### ✅ 设计优势
1. **模块化设计**：各组件独立，易于维护和扩展
2. **可配置性**：支持多种清洗和提取选项
3. **批量处理**：支持大规模数据处理
4. **状态管理**：可保存和恢复处理状态

### ✅ 功能优势
1. **专业性强**：基于PHQ-9问卷的抑郁特征
2. **全面性**：涵盖语言学、情感、社交媒体特征
3. **标准化**：自动特征标准化和标签编码
4. **可扩展**：易于添加新的特征类型

### ✅ 实用优势
1. **易用性**：简单的API接口
2. **健壮性**：完善的错误处理
3. **效率性**：优化的批量处理算法
4. **兼容性**：支持多种数据格式

---

## 📝 总结

数据预处理模块是整个抑郁风险预警系统的基础，它将原始的社交媒体文本转换为机器学习模型可以直接使用的标准化特征。通过三个核心组件的协同工作，实现了从数据清洗到特征提取的完整流程，为后续的模型训练和预测提供了高质量的数据基础。

**核心价值**：
- 🎯 **精准识别**：基于专业医学标准的抑郁特征提取
- 🔄 **自动化处理**：从原始数据到训练数据的全自动转换
- 📊 **丰富特征**：48个多维特征全面描述用户状态
- 🛡️ **质量保证**：多重清洗和验证确保数据质量
