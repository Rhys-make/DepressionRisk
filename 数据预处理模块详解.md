# 数据预处理模块详解

## 📋 目录
- [模块概述](#模块概述)
- [架构设计](#架构设计)
- [核心组件](#核心组件)
- [功能详解](#功能详解)
- [设计原理](#设计原理)
- [使用示例](#使用示例)
- [最佳实践](#最佳实践)
- [常见问题](#常见问题)

## 🎯 模块概述

数据预处理模块是整个机器学习项目的**基础工程**，负责将原始的社交媒体文本转换为机器学习模型可以直接使用的数值特征。该模块采用模块化设计，确保代码的可维护性和可扩展性。

### 主要功能
- **文本清洗** - 清理和标准化文本
- **特征提取** - 从文本中提取数值特征
- **数据标准化** - 统一特征尺度
- **标签编码** - 将分类标签转换为数值
- **数据持久化** - 保存和加载处理后的数据

## 🏗️ 架构设计

```
src/data_processing/
├── __init__.py          # 模块入口
├── text_cleaner.py      # 文本清洗器
├── feature_extractor.py # 特征提取器
└── preprocessor.py      # 数据预处理器（主控制器）
```

### 设计原则
1. **单一职责原则** - 每个类只负责一个特定功能
2. **开闭原则** - 对扩展开放，对修改封闭
3. **依赖倒置原则** - 高层模块不依赖低层模块
4. **接口隔离原则** - 提供简洁的接口

## 🔧 核心组件

### 1. TextCleaner - 文本清洗器

#### 功能作用
- **文本标准化**：将各种格式的社交媒体文本转换为统一的格式
- **噪声去除**：移除对分析无用的信息（URL、邮箱、电话号码等）
- **表情符号处理**：将表情符号转换为可理解的文本标签

#### 核心特性
```python
class TextCleaner:
    def __init__(self):
        # 表情符号映射表
        self.emoticon_map = {
            ':)': ' [happy] ', ':-)': ' [happy] ',
            ':(': ' [sad] ', ':-(': ' [sad] ',
            ';)': ' [wink] ', ';-)': ' [wink] ',
            # ... 更多映射
        }
        
        # 正则表达式模式
        self.patterns = {
            'url': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'hashtag': r'#\w+',
            'mention': r'@\w+',
            # ... 更多模式
        }
```

#### 主要方法

**`clean_text()` - 文本清洗**
```python
def clean_text(self, text: str, remove_urls: bool = True, ...) -> str:
    # 1. HTML解码
    text = html.unescape(text)
    
    # 2. Unicode标准化
    text = unicodedata.normalize('NFKC', text)
    
    # 3. 移除各种模式
    if remove_urls:
        text = self.compiled_patterns['url'].sub(' [URL] ', text)
    
    # 4. 表情符号标准化
    if normalize_emoticons:
        text = self._normalize_emoticons(text)
    
    # 5. 重复字符处理
    if remove_repeated_chars:
        text = self.compiled_patterns['repeated_chars'].sub(r'\1\1', text)
    
    # 6. 转换为小写并清理空格
    if lowercase:
        text = text.lower()
    text = self.compiled_patterns['extra_spaces'].sub(' ', text)
    
    return text.strip()
```

#### 设计原理
1. **社交媒体文本特点**：
   - 包含大量非标准格式（表情符号、缩写、网络用语）
   - 噪声信息多（URL、邮箱、电话号码）
   - 重复字符和单词常见

2. **机器学习需求**：
   - 模型需要统一的文本格式
   - 表情符号包含情感信息，不能简单删除
   - 噪声信息会干扰模型学习

### 2. FeatureExtractor - 特征提取器

#### 功能作用
- **特征工程**：将文本转换为机器学习模型可用的数值特征
- **多维度分析**：从语言学、情感、抑郁症状、社交媒体等多个角度提取特征
- **特征标准化**：确保所有特征都是数值型，便于模型处理

#### 核心特性
```python
class FeatureExtractor:
    def __init__(self):
        # 抑郁相关词汇（基于PHQ-9问卷）
        self.depression_keywords = {
            '情绪低落': ['sad', 'depressed', 'down', 'blue', 'unhappy', 'miserable', 'hopeless'],
            '兴趣丧失': ['uninterested', 'bored', 'no_interest', 'nothing_matters', 'empty'],
            '睡眠问题': ['insomnia', 'sleep', 'tired', 'exhausted', 'fatigue', 'restless'],
            '自杀想法': ['suicide', 'kill_myself', 'death', 'die', 'end_it_all', 'better_off_dead'],
            # ... 更多类别
        }
        
        # 情感词汇
        self.emotion_words = {
            'positive': ['happy', 'joy', 'excited', 'love', 'great', 'wonderful'],
            'negative': ['sad', 'angry', 'frustrated', 'disappointed', 'hate', 'terrible'],
            'neutral': ['okay', 'fine', 'normal', 'average', 'usual']
        }
```

#### 主要方法

**`extract_linguistic_features()` - 语言学特征**
```python
def extract_linguistic_features(self, text: str) -> Dict[str, float]:
    features = {}
    
    # 基本统计特征
    features['text_length'] = len(text)  # 文本长度
    features['word_count'] = len(text.split())  # 单词数量
    features['avg_word_length'] = np.mean([len(word) for word in text.split()])  # 平均单词长度
    
    # 词汇多样性
    words = text.lower().split()
    if words:
        features['unique_words'] = len(set(words))  # 唯一单词数
        features['lexical_diversity'] = features['unique_words'] / features['word_count']  # 词汇多样性
    
    # 标点符号特征
    features['exclamation_count'] = len(re.findall(r'!+', text))  # 感叹号数量
    features['question_count'] = len(re.findall(r'\?+', text))  # 问号数量
    features['ellipsis_count'] = len(re.findall(r'\.{3,}', text))  # 省略号数量
    
    return features
```

**`extract_depression_features()` - 抑郁特征**
```python
def extract_depression_features(self, text: str) -> Dict[str, float]:
    features = {}
    text_lower = text.lower()
    
    # 计算各类抑郁关键词的出现次数
    for category, keywords in self.depression_keywords.items():
        count = 0
        for keyword in keywords:
            # 使用单词边界匹配，避免部分匹配
            pattern = r'\b' + re.escape(keyword) + r'\b'
            count += len(re.findall(pattern, text_lower))
        features[f'depression_{category}_count'] = count
    
    # 抑郁词汇密度
    word_count = len(text.split())
    features['depression_word_density'] = features['total_depression_words'] / word_count
    
    return features
```

**`extract_emotion_features()` - 情感特征**
```python
def extract_emotion_features(self, text: str) -> Dict[str, float]:
    features = {}
    text_lower = text.lower()
    
    # 计算各类情感词汇的出现次数
    for emotion, words in self.emotion_words.items():
        count = sum(text_lower.count(word) for word in words)
        features[f'{emotion}_emotion_count'] = count
    
    # 情感极性（-1到1之间）
    positive_count = features['positive_emotion_count']
    negative_count = features['negative_emotion_count']
    total_emotion = positive_count + negative_count
    
    if total_emotion > 0:
        features['emotion_polarity'] = (positive_count - negative_count) / total_emotion
    else:
        features['emotion_polarity'] = 0
    
    return features
```

#### 设计原理
1. **多维度特征**：
   - **语言学特征**：反映文本的基本结构和复杂度
   - **抑郁特征**：基于临床心理学理论（PHQ-9问卷）
   - **情感特征**：捕捉文本的情感倾向
   - **社交媒体特征**：适应社交媒体文本的特殊性

2. **特征质量保证**：
   - 使用单词边界匹配避免误匹配
   - 提供默认特征处理异常情况
   - 确保所有特征都是数值型

### 3. DataProcessor - 数据预处理器

#### 功能作用
- **流程控制**：协调文本清洗和特征提取的整个流程
- **数据管理**：处理数据的加载、保存、分割
- **特征标准化**：使用StandardScaler标准化特征
- **标签编码**：使用LabelEncoder编码分类标签
- **状态持久化**：保存和加载处理器的状态

#### 核心特性
```python
class DataProcessor:
    def __init__(self, text_column: str = 'text', label_column: str = 'label', 
                 min_text_length: int = 10, max_text_length: int = 1000):
        self.text_column = text_column  # 文本列名
        self.label_column = label_column  # 标签列名
        self.min_text_length = min_text_length  # 最小文本长度
        self.max_text_length = max_text_length  # 最大文本长度
        
        # 初始化组件
        self.preprocessor = TextPreprocessor(...)
        
        # 保存处理参数
        self.scaler = None  # 特征标准化器
        self.label_encoder = None  # 标签编码器
        self.feature_names = None  # 特征名称列表
        self.feature_columns = None  # 可控制使用的特征列
```

#### 主要方法

**`process_social_media_data()` - 数据处理流程**
```python
def process_social_media_data(self, data: pd.DataFrame) -> pd.DataFrame:
    # 1. 数据清洗
    data = data.dropna(subset=[self.text_column])  # 移除空文本
    data = data[data[self.text_column].str.strip() != '']  # 移除空白文本
    
    # 2. 文本长度过滤
    data['text_length'] = data[self.text_column].str.len()
    data = data[
        (data['text_length'] >= self.min_text_length) & 
        (data['text_length'] <= self.max_text_length)
    ]
    
    # 3. 文本清洗
    data['cleaned_text'] = self.preprocessor.clean_batch(data[self.text_column].tolist())
    
    # 4. 特征提取
    features_list = self.preprocessor.extract_batch_features(data['cleaned_text'].tolist())
    
    # 5. 特征整合（关键修复：索引对齐）
    if features_list:
        features_df = pd.DataFrame(features_list)
        data = data.reset_index(drop=True)  # 重置索引
        features_df.index = data.index  # 确保索引对齐
        data = pd.concat([data, features_df], axis=1)  # 合并数据
        data[features_df.columns] = data[features_df.columns].fillna(0.0)  # 处理NaN值
        self.feature_names = list(features_df.columns)  # 保存特征名称
```

**`prepare_features()` - 特征准备**
```python
def prepare_features(self, data: pd.DataFrame, feature_columns: Optional[List[str]] = None,
                    label_column: Optional[str] = None, fit_scaler: bool = True) -> Tuple[np.ndarray, Optional[np.ndarray]]:
    # 1. 确定特征列
    if feature_columns is None:
        feature_columns = self.feature_columns or self.feature_names or []
    
    # 2. 检查特征列是否存在
    missing_cols = [col for col in feature_columns if col not in data.columns]
    if missing_cols:
        raise ValueError(f"数据中缺少特征列: {missing_cols}")
    
    # 3. 提取特征
    X = data[feature_columns].values
    
    # 4. 处理NaN值
    if np.isnan(X).any():
        X = np.nan_to_num(X, nan=0.0)
    
    # 5. 特征标准化
    if fit_scaler or self.scaler is None:
        self.scaler = StandardScaler()
        X = self.scaler.fit_transform(X)  # 训练时拟合
    else:
        X = self.scaler.transform(X)  # 预测时转换
    
    # 6. 标签编码
    y = None
    if label_column and label_column in data.columns:
        if self.label_encoder is None:
            self.label_encoder = LabelEncoder()
            y = self.label_encoder.fit_transform(data[label_column])  # 训练时拟合
        else:
            y = self.label_encoder.transform(data[label_column])  # 预测时转换
    
    return X, y
```

**`save_processor()` 和 `load_processor()` - 状态持久化**
```python
def save_processor(self, file_path: Union[str, Path]):
    # 保存处理器状态
    processor_state = {
        'text_column': self.text_column,
        'feature_names': self.feature_names,
        'scaler': self.scaler,  # 保存标准化器
        'label_encoder': self.label_encoder  # 保存标签编码器
    }
    
    # 保存pickle格式
    with open(file_path, 'wb') as f:
        pickle.dump(processor_state, f)
    
    # 同时保存JSON格式（跨语言兼容）
    json_state = {
        'scaler_mean': self.scaler.mean_.tolist() if self.scaler else None,
        'scaler_scale': self.scaler.scale_.tolist() if self.scaler else None,
        'label_classes': self.label_encoder.classes_.tolist() if self.label_encoder else None
    }
    # 保存JSON...

def load_processor(self, file_path: Union[str, Path]):
    # 加载处理器状态
    with open(file_path, 'rb') as f:
        processor_state = pickle.load(f)
    
    # 恢复所有状态
    self.text_column = processor_state['text_column']
    self.feature_names = processor_state['feature_names']
    self.scaler = processor_state['scaler']  # 恢复标准化器
    self.label_encoder = processor_state['label_encoder']  # 恢复标签编码器
```

#### 设计原理
1. **流程标准化**：
   - 确保数据处理流程的一致性
   - 避免数据泄露（训练和测试使用相同的标准化器）
   - 保证特征维度的一致性

2. **状态管理**：
   - 保存处理器的状态，确保预测时使用相同的参数
   - 支持增量训练和模型更新
   - 提供跨语言兼容性（JSON格式）

3. **错误处理**：
   - 处理NaN值和异常情况
   - 提供详细的日志信息
   - 确保数据完整性

## 🎯 功能详解

### 文本清洗功能

#### 支持的清洗操作
1. **HTML解码** - 解码HTML实体字符
2. **Unicode标准化** - 统一字符编码
3. **URL移除** - 替换为 `[URL]` 标记
4. **邮箱移除** - 替换为 `[EMAIL]` 标记
5. **电话号码移除** - 替换为 `[PHONE]` 标记
6. **表情符号标准化** - 转换为语义标签
7. **重复字符处理** - 限制重复字符数量
8. **重复单词处理** - 移除重复单词
9. **大小写转换** - 统一为小写
10. **空格清理** - 规范化空格

#### 表情符号映射
```python
emoticon_map = {
    ':)': ' [happy] ', ':-)': ' [happy] ', ':D': ' [happy] ',
    ':(': ' [sad] ', ':-(': ' [sad] ', ':((': ' [sad] ',
    ';)': ' [wink] ', ';-)': ' [wink] ',
    ':P': ' [tongue] ', ':-P': ' [tongue] ',
    '<3': ' [heart] ', '</3': ' [broken_heart] ',
    'T_T': ' [crying] ', 'T.T': ' [crying] ',
    '^_^': ' [smile] ', '^^': ' [smile] ',
    '>:(': ' [angry] ', '>:((': ' [angry] ',
    ':|': ' [neutral] ', ':-|': ' [neutral] '
}
```

### 特征提取功能

#### 语言学特征（12个）
- `text_length` - 文本长度
- `word_count` - 单词数量
- `char_count` - 字符数量
- `avg_word_length` - 平均单词长度
- `sentence_count` - 句子数量
- `avg_sentence_length` - 平均句子长度
- `unique_words` - 唯一单词数
- `lexical_diversity` - 词汇多样性
- `type_token_ratio` - 类型标记比
- `exclamation_count` - 感叹号数量
- `question_count` - 问号数量
- `ellipsis_count` - 省略号数量
- `caps_words_count` - 大写单词数量
- `uppercase_ratio` - 大写字母比例

#### 抑郁特征（11个）
- `depression_情绪低落_count` - 情绪低落相关词汇数量
- `depression_兴趣丧失_count` - 兴趣丧失相关词汇数量
- `depression_睡眠问题_count` - 睡眠问题相关词汇数量
- `depression_食欲变化_count` - 食欲变化相关词汇数量
- `depression_注意力问题_count` - 注意力问题相关词汇数量
- `depression_自我评价_count` - 自我评价相关词汇数量
- `depression_自杀想法_count` - 自杀想法相关词汇数量
- `depression_焦虑症状_count` - 焦虑症状相关词汇数量
- `depression_身体症状_count` - 身体症状相关词汇数量
- `depression_社交退缩_count` - 社交退缩相关词汇数量
- `total_depression_words` - 总抑郁词汇数量
- `depression_word_density` - 抑郁词汇密度
- `depression_categories` - 抑郁词汇类别数

#### 情感特征（6个）
- `positive_emotion_count` - 积极情感词汇数量
- `negative_emotion_count` - 消极情感词汇数量
- `neutral_emotion_count` - 中性情感词汇数量
- `total_emotion_words` - 总情感词汇数量
- `emotion_word_density` - 情感词汇密度
- `emotion_polarity` - 情感极性（-1到1）

#### 社交媒体特征（8个）
- `hashtag_count` - 话题标签数量
- `mention_count` - @提及数量
- `url_count` - URL数量
- `emoticon_count` - 表情符号数量
- `repeated_char_count` - 重复字符数量
- `repeated_word_count` - 重复单词数量
- `exclamation_density` - 感叹号密度
- `question_density` - 问号密度

### 数据标准化功能

#### StandardScaler标准化
```python
# 标准化公式：z = (x - μ) / σ
# 其中 μ 是均值，σ 是标准差
X_scaled = (X - X.mean()) / X.std()
```

#### LabelEncoder编码
```python
# 将分类标签转换为数值
# 例如：['低风险', '高风险'] -> [0, 1]
y_encoded = label_encoder.fit_transform(y)
```

## 🔬 设计原理

### 1. 模块化设计
- **单一职责**：每个类只负责一个特定功能
- **松耦合**：组件之间通过接口交互
- **高内聚**：相关功能集中在同一个类中

### 2. 可扩展性设计
- **插件式架构**：可以轻松添加新的特征提取器
- **配置驱动**：通过参数控制处理行为
- **接口标准化**：统一的接口便于扩展

### 3. 鲁棒性设计
- **异常处理**：处理各种异常情况
- **默认值机制**：提供合理的默认值
- **数据验证**：确保数据质量

### 4. 性能优化
- **批量处理**：支持批量操作提高效率
- **正则表达式编译**：预编译正则表达式
- **内存管理**：合理管理内存使用

## 💻 使用示例

### 基本使用流程

```python
from src.data_processing.preprocessor import DataProcessor

# 1. 初始化数据处理器
processor = DataProcessor(
    text_column='text',
    label_column='label',
    min_text_length=10,
    max_text_length=1000
)

# 2. 加载数据
data = processor.load_data('data/raw/sample_data.csv')

# 3. 处理数据
processed_data = processor.process_social_media_data(data)

# 4. 准备特征
X_train, y_train = processor.prepare_features(
    processed_data, 
    label_column='label', 
    fit_scaler=True
)

# 5. 保存处理器状态
processor.save_processor('models/data_processor.pkl')
```

### 预测时使用

```python
# 1. 加载处理器状态
processor = DataProcessor()
processor.load_processor('models/data_processor.pkl')

# 2. 处理新数据
new_data = pd.DataFrame({'text': ['I feel sad today']})
processed_new_data = processor.process_social_media_data(new_data)

# 3. 准备特征（不拟合标准化器）
X_new, _ = processor.prepare_features(
    processed_new_data, 
    fit_scaler=False
)

# 4. 使用模型预测
prediction = model.predict(X_new)
```

### 批量处理示例

```python
# 批量清洗文本
texts = [
    "I am so happy today!!! 😊 #blessed @friend",
    "I feel so sad and hopeless... :( #depression",
    "RT @user: This is a retweet with http://example.com"
]

cleaner = TextCleaner()
cleaned_texts = cleaner.clean_batch(texts)

# 批量提取特征
extractor = FeatureExtractor()
features_list = extractor.extract_batch_features(cleaned_texts)
```

### 自定义特征列

```python
# 设置要使用的特征列
processor.set_feature_columns([
    'text_length', 'word_count', 'avg_word_length',
    'depression_情绪低落_count', 'depression_自杀想法_count',
    'positive_emotion_count', 'negative_emotion_count',
    'emotion_polarity'
])

# 使用指定的特征列
X_train, y_train = processor.prepare_features(
    processed_data, 
    label_column='label', 
    fit_scaler=True
)
```

## 🛠️ 最佳实践

### 1. 数据质量保证
- **文本长度过滤**：移除过短或过长的文本
- **空值处理**：移除空文本和空白文本
- **重复数据检查**：识别和处理重复数据

### 2. 特征工程最佳实践
- **特征选择**：选择最相关的特征
- **特征缩放**：确保特征在相同尺度
- **特征验证**：验证特征的质量和有效性

### 3. 模型一致性保证
- **状态保存**：保存处理器的完整状态
- **版本控制**：记录处理器的版本信息
- **测试验证**：定期测试处理器的功能

### 4. 性能优化建议
- **批量处理**：使用批量操作提高效率
- **内存管理**：及时释放不需要的数据
- **缓存机制**：缓存中间结果避免重复计算

## ❓ 常见问题

### Q1: 为什么要进行文本清洗？
**A:** 社交媒体文本包含大量噪声信息（URL、表情符号、重复字符等），这些信息会干扰机器学习模型的学习。文本清洗可以：
- 统一文本格式
- 移除无关信息
- 保留有用信息（如表情符号的情感信息）

### Q2: 特征提取为什么选择这些特征？
**A:** 这些特征是基于以下考虑选择的：
- **语言学特征**：反映文本的基本结构和复杂度
- **抑郁特征**：基于临床心理学理论（PHQ-9问卷）
- **情感特征**：捕捉文本的情感倾向
- **社交媒体特征**：适应社交媒体文本的特殊性

### Q3: 为什么要保存处理器状态？
**A:** 保存处理器状态是为了确保：
- **一致性**：训练和预测时使用相同的参数
- **可重现性**：可以重现相同的处理结果
- **部署便利**：便于模型部署和更新

### Q4: 如何处理新出现的词汇？
**A:** 可以通过以下方式处理：
- **扩展词汇表**：定期更新关键词列表
- **动态特征**：使用词向量等动态特征
- **迁移学习**：使用预训练模型

### Q5: 特征维度不匹配怎么办？
**A:** 可以通过以下方式解决：
- **特征对齐**：确保训练和预测时使用相同的特征
- **默认值填充**：为缺失特征提供默认值
- **特征选择**：使用固定的特征子集

## 📚 参考资料

1. **PHQ-9问卷**：抑郁症筛查工具
2. **文本预处理技术**：NLP基础技术
3. **特征工程方法**：机器学习特征工程
4. **数据标准化**：机器学习数据预处理
5. **模型持久化**：机器学习模型部署

---

*本文档详细介绍了数据预处理模块的设计原理、功能实现和使用方法，为项目的开发和维护提供了全面的技术指导。*
