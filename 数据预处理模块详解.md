# æ•°æ®é¢„å¤„ç†æ¨¡å—è¯¦è§£

## ğŸ“‹ ç›®å½•
- [æ¨¡å—æ¦‚è¿°](#æ¨¡å—æ¦‚è¿°)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
- [åŠŸèƒ½è¯¦è§£](#åŠŸèƒ½è¯¦è§£)
- [è®¾è®¡åŸç†](#è®¾è®¡åŸç†)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## ğŸ¯ æ¨¡å—æ¦‚è¿°

æ•°æ®é¢„å¤„ç†æ¨¡å—æ˜¯æ•´ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®çš„**åŸºç¡€å·¥ç¨‹**ï¼Œè´Ÿè´£å°†åŸå§‹çš„ç¤¾äº¤åª’ä½“æ–‡æœ¬è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨çš„æ•°å€¼ç‰¹å¾ã€‚è¯¥æ¨¡å—é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œç¡®ä¿ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¯æ‰©å±•æ€§ã€‚

### ä¸»è¦åŠŸèƒ½
- **æ–‡æœ¬æ¸…æ´—** - æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬
- **ç‰¹å¾æå–** - ä»æ–‡æœ¬ä¸­æå–æ•°å€¼ç‰¹å¾
- **æ•°æ®æ ‡å‡†åŒ–** - ç»Ÿä¸€ç‰¹å¾å°ºåº¦
- **æ ‡ç­¾ç¼–ç ** - å°†åˆ†ç±»æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
- **æ•°æ®æŒä¹…åŒ–** - ä¿å­˜å’ŒåŠ è½½å¤„ç†åçš„æ•°æ®

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
src/data_processing/
â”œâ”€â”€ __init__.py          # æ¨¡å—å…¥å£
â”œâ”€â”€ text_cleaner.py      # æ–‡æœ¬æ¸…æ´—å™¨
â”œâ”€â”€ feature_extractor.py # ç‰¹å¾æå–å™¨
â””â”€â”€ preprocessor.py      # æ•°æ®é¢„å¤„ç†å™¨ï¼ˆä¸»æ§åˆ¶å™¨ï¼‰
```

### è®¾è®¡åŸåˆ™
1. **å•ä¸€èŒè´£åŸåˆ™** - æ¯ä¸ªç±»åªè´Ÿè´£ä¸€ä¸ªç‰¹å®šåŠŸèƒ½
2. **å¼€é—­åŸåˆ™** - å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å°é—­
3. **ä¾èµ–å€’ç½®åŸåˆ™** - é«˜å±‚æ¨¡å—ä¸ä¾èµ–ä½å±‚æ¨¡å—
4. **æ¥å£éš”ç¦»åŸåˆ™** - æä¾›ç®€æ´çš„æ¥å£

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. TextCleaner - æ–‡æœ¬æ¸…æ´—å™¨

#### åŠŸèƒ½ä½œç”¨
- **æ–‡æœ¬æ ‡å‡†åŒ–**ï¼šå°†å„ç§æ ¼å¼çš„ç¤¾äº¤åª’ä½“æ–‡æœ¬è½¬æ¢ä¸ºç»Ÿä¸€çš„æ ¼å¼
- **å™ªå£°å»é™¤**ï¼šç§»é™¤å¯¹åˆ†ææ— ç”¨çš„ä¿¡æ¯ï¼ˆURLã€é‚®ç®±ã€ç”µè¯å·ç ç­‰ï¼‰
- **è¡¨æƒ…ç¬¦å·å¤„ç†**ï¼šå°†è¡¨æƒ…ç¬¦å·è½¬æ¢ä¸ºå¯ç†è§£çš„æ–‡æœ¬æ ‡ç­¾

#### æ ¸å¿ƒç‰¹æ€§
```python
class TextCleaner:
    def __init__(self):
        # è¡¨æƒ…ç¬¦å·æ˜ å°„è¡¨
        self.emoticon_map = {
            ':)': ' [happy] ', ':-)': ' [happy] ',
            ':(': ' [sad] ', ':-(': ' [sad] ',
            ';)': ' [wink] ', ';-)': ' [wink] ',
            # ... æ›´å¤šæ˜ å°„
        }
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.patterns = {
            'url': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'hashtag': r'#\w+',
            'mention': r'@\w+',
            # ... æ›´å¤šæ¨¡å¼
        }
```

#### ä¸»è¦æ–¹æ³•

**`clean_text()` - æ–‡æœ¬æ¸…æ´—**
```python
def clean_text(self, text: str, remove_urls: bool = True, ...) -> str:
    # 1. HTMLè§£ç 
    text = html.unescape(text)
    
    # 2. Unicodeæ ‡å‡†åŒ–
    text = unicodedata.normalize('NFKC', text)
    
    # 3. ç§»é™¤å„ç§æ¨¡å¼
    if remove_urls:
        text = self.compiled_patterns['url'].sub(' [URL] ', text)
    
    # 4. è¡¨æƒ…ç¬¦å·æ ‡å‡†åŒ–
    if normalize_emoticons:
        text = self._normalize_emoticons(text)
    
    # 5. é‡å¤å­—ç¬¦å¤„ç†
    if remove_repeated_chars:
        text = self.compiled_patterns['repeated_chars'].sub(r'\1\1', text)
    
    # 6. è½¬æ¢ä¸ºå°å†™å¹¶æ¸…ç†ç©ºæ ¼
    if lowercase:
        text = text.lower()
    text = self.compiled_patterns['extra_spaces'].sub(' ', text)
    
    return text.strip()
```

#### è®¾è®¡åŸç†
1. **ç¤¾äº¤åª’ä½“æ–‡æœ¬ç‰¹ç‚¹**ï¼š
   - åŒ…å«å¤§é‡éæ ‡å‡†æ ¼å¼ï¼ˆè¡¨æƒ…ç¬¦å·ã€ç¼©å†™ã€ç½‘ç»œç”¨è¯­ï¼‰
   - å™ªå£°ä¿¡æ¯å¤šï¼ˆURLã€é‚®ç®±ã€ç”µè¯å·ç ï¼‰
   - é‡å¤å­—ç¬¦å’Œå•è¯å¸¸è§

2. **æœºå™¨å­¦ä¹ éœ€æ±‚**ï¼š
   - æ¨¡å‹éœ€è¦ç»Ÿä¸€çš„æ–‡æœ¬æ ¼å¼
   - è¡¨æƒ…ç¬¦å·åŒ…å«æƒ…æ„Ÿä¿¡æ¯ï¼Œä¸èƒ½ç®€å•åˆ é™¤
   - å™ªå£°ä¿¡æ¯ä¼šå¹²æ‰°æ¨¡å‹å­¦ä¹ 

### 2. FeatureExtractor - ç‰¹å¾æå–å™¨

#### åŠŸèƒ½ä½œç”¨
- **ç‰¹å¾å·¥ç¨‹**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹å¯ç”¨çš„æ•°å€¼ç‰¹å¾
- **å¤šç»´åº¦åˆ†æ**ï¼šä»è¯­è¨€å­¦ã€æƒ…æ„Ÿã€æŠ‘éƒç—‡çŠ¶ã€ç¤¾äº¤åª’ä½“ç­‰å¤šä¸ªè§’åº¦æå–ç‰¹å¾
- **ç‰¹å¾æ ‡å‡†åŒ–**ï¼šç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼å‹ï¼Œä¾¿äºæ¨¡å‹å¤„ç†

#### æ ¸å¿ƒç‰¹æ€§
```python
class FeatureExtractor:
    def __init__(self):
        # æŠ‘éƒç›¸å…³è¯æ±‡ï¼ˆåŸºäºPHQ-9é—®å·ï¼‰
        self.depression_keywords = {
            'æƒ…ç»ªä½è½': ['sad', 'depressed', 'down', 'blue', 'unhappy', 'miserable', 'hopeless'],
            'å…´è¶£ä¸§å¤±': ['uninterested', 'bored', 'no_interest', 'nothing_matters', 'empty'],
            'ç¡çœ é—®é¢˜': ['insomnia', 'sleep', 'tired', 'exhausted', 'fatigue', 'restless'],
            'è‡ªæ€æƒ³æ³•': ['suicide', 'kill_myself', 'death', 'die', 'end_it_all', 'better_off_dead'],
            # ... æ›´å¤šç±»åˆ«
        }
        
        # æƒ…æ„Ÿè¯æ±‡
        self.emotion_words = {
            'positive': ['happy', 'joy', 'excited', 'love', 'great', 'wonderful'],
            'negative': ['sad', 'angry', 'frustrated', 'disappointed', 'hate', 'terrible'],
            'neutral': ['okay', 'fine', 'normal', 'average', 'usual']
        }
```

#### ä¸»è¦æ–¹æ³•

**`extract_linguistic_features()` - è¯­è¨€å­¦ç‰¹å¾**
```python
def extract_linguistic_features(self, text: str) -> Dict[str, float]:
    features = {}
    
    # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
    features['text_length'] = len(text)  # æ–‡æœ¬é•¿åº¦
    features['word_count'] = len(text.split())  # å•è¯æ•°é‡
    features['avg_word_length'] = np.mean([len(word) for word in text.split()])  # å¹³å‡å•è¯é•¿åº¦
    
    # è¯æ±‡å¤šæ ·æ€§
    words = text.lower().split()
    if words:
        features['unique_words'] = len(set(words))  # å”¯ä¸€å•è¯æ•°
        features['lexical_diversity'] = features['unique_words'] / features['word_count']  # è¯æ±‡å¤šæ ·æ€§
    
    # æ ‡ç‚¹ç¬¦å·ç‰¹å¾
    features['exclamation_count'] = len(re.findall(r'!+', text))  # æ„Ÿå¹å·æ•°é‡
    features['question_count'] = len(re.findall(r'\?+', text))  # é—®å·æ•°é‡
    features['ellipsis_count'] = len(re.findall(r'\.{3,}', text))  # çœç•¥å·æ•°é‡
    
    return features
```

**`extract_depression_features()` - æŠ‘éƒç‰¹å¾**
```python
def extract_depression_features(self, text: str) -> Dict[str, float]:
    features = {}
    text_lower = text.lower()
    
    # è®¡ç®—å„ç±»æŠ‘éƒå…³é”®è¯çš„å‡ºç°æ¬¡æ•°
    for category, keywords in self.depression_keywords.items():
        count = 0
        for keyword in keywords:
            # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
            pattern = r'\b' + re.escape(keyword) + r'\b'
            count += len(re.findall(pattern, text_lower))
        features[f'depression_{category}_count'] = count
    
    # æŠ‘éƒè¯æ±‡å¯†åº¦
    word_count = len(text.split())
    features['depression_word_density'] = features['total_depression_words'] / word_count
    
    return features
```

**`extract_emotion_features()` - æƒ…æ„Ÿç‰¹å¾**
```python
def extract_emotion_features(self, text: str) -> Dict[str, float]:
    features = {}
    text_lower = text.lower()
    
    # è®¡ç®—å„ç±»æƒ…æ„Ÿè¯æ±‡çš„å‡ºç°æ¬¡æ•°
    for emotion, words in self.emotion_words.items():
        count = sum(text_lower.count(word) for word in words)
        features[f'{emotion}_emotion_count'] = count
    
    # æƒ…æ„Ÿææ€§ï¼ˆ-1åˆ°1ä¹‹é—´ï¼‰
    positive_count = features['positive_emotion_count']
    negative_count = features['negative_emotion_count']
    total_emotion = positive_count + negative_count
    
    if total_emotion > 0:
        features['emotion_polarity'] = (positive_count - negative_count) / total_emotion
    else:
        features['emotion_polarity'] = 0
    
    return features
```

#### è®¾è®¡åŸç†
1. **å¤šç»´åº¦ç‰¹å¾**ï¼š
   - **è¯­è¨€å­¦ç‰¹å¾**ï¼šåæ˜ æ–‡æœ¬çš„åŸºæœ¬ç»“æ„å’Œå¤æ‚åº¦
   - **æŠ‘éƒç‰¹å¾**ï¼šåŸºäºä¸´åºŠå¿ƒç†å­¦ç†è®ºï¼ˆPHQ-9é—®å·ï¼‰
   - **æƒ…æ„Ÿç‰¹å¾**ï¼šæ•æ‰æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘
   - **ç¤¾äº¤åª’ä½“ç‰¹å¾**ï¼šé€‚åº”ç¤¾äº¤åª’ä½“æ–‡æœ¬çš„ç‰¹æ®Šæ€§

2. **ç‰¹å¾è´¨é‡ä¿è¯**ï¼š
   - ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…é¿å…è¯¯åŒ¹é…
   - æä¾›é»˜è®¤ç‰¹å¾å¤„ç†å¼‚å¸¸æƒ…å†µ
   - ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼å‹

### 3. DataProcessor - æ•°æ®é¢„å¤„ç†å™¨

#### åŠŸèƒ½ä½œç”¨
- **æµç¨‹æ§åˆ¶**ï¼šåè°ƒæ–‡æœ¬æ¸…æ´—å’Œç‰¹å¾æå–çš„æ•´ä¸ªæµç¨‹
- **æ•°æ®ç®¡ç†**ï¼šå¤„ç†æ•°æ®çš„åŠ è½½ã€ä¿å­˜ã€åˆ†å‰²
- **ç‰¹å¾æ ‡å‡†åŒ–**ï¼šä½¿ç”¨StandardScaleræ ‡å‡†åŒ–ç‰¹å¾
- **æ ‡ç­¾ç¼–ç **ï¼šä½¿ç”¨LabelEncoderç¼–ç åˆ†ç±»æ ‡ç­¾
- **çŠ¶æ€æŒä¹…åŒ–**ï¼šä¿å­˜å’ŒåŠ è½½å¤„ç†å™¨çš„çŠ¶æ€

#### æ ¸å¿ƒç‰¹æ€§
```python
class DataProcessor:
    def __init__(self, text_column: str = 'text', label_column: str = 'label', 
                 min_text_length: int = 10, max_text_length: int = 1000):
        self.text_column = text_column  # æ–‡æœ¬åˆ—å
        self.label_column = label_column  # æ ‡ç­¾åˆ—å
        self.min_text_length = min_text_length  # æœ€å°æ–‡æœ¬é•¿åº¦
        self.max_text_length = max_text_length  # æœ€å¤§æ–‡æœ¬é•¿åº¦
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.preprocessor = TextPreprocessor(...)
        
        # ä¿å­˜å¤„ç†å‚æ•°
        self.scaler = None  # ç‰¹å¾æ ‡å‡†åŒ–å™¨
        self.label_encoder = None  # æ ‡ç­¾ç¼–ç å™¨
        self.feature_names = None  # ç‰¹å¾åç§°åˆ—è¡¨
        self.feature_columns = None  # å¯æ§åˆ¶ä½¿ç”¨çš„ç‰¹å¾åˆ—
```

#### ä¸»è¦æ–¹æ³•

**`process_social_media_data()` - æ•°æ®å¤„ç†æµç¨‹**
```python
def process_social_media_data(self, data: pd.DataFrame) -> pd.DataFrame:
    # 1. æ•°æ®æ¸…æ´—
    data = data.dropna(subset=[self.text_column])  # ç§»é™¤ç©ºæ–‡æœ¬
    data = data[data[self.text_column].str.strip() != '']  # ç§»é™¤ç©ºç™½æ–‡æœ¬
    
    # 2. æ–‡æœ¬é•¿åº¦è¿‡æ»¤
    data['text_length'] = data[self.text_column].str.len()
    data = data[
        (data['text_length'] >= self.min_text_length) & 
        (data['text_length'] <= self.max_text_length)
    ]
    
    # 3. æ–‡æœ¬æ¸…æ´—
    data['cleaned_text'] = self.preprocessor.clean_batch(data[self.text_column].tolist())
    
    # 4. ç‰¹å¾æå–
    features_list = self.preprocessor.extract_batch_features(data['cleaned_text'].tolist())
    
    # 5. ç‰¹å¾æ•´åˆï¼ˆå…³é”®ä¿®å¤ï¼šç´¢å¼•å¯¹é½ï¼‰
    if features_list:
        features_df = pd.DataFrame(features_list)
        data = data.reset_index(drop=True)  # é‡ç½®ç´¢å¼•
        features_df.index = data.index  # ç¡®ä¿ç´¢å¼•å¯¹é½
        data = pd.concat([data, features_df], axis=1)  # åˆå¹¶æ•°æ®
        data[features_df.columns] = data[features_df.columns].fillna(0.0)  # å¤„ç†NaNå€¼
        self.feature_names = list(features_df.columns)  # ä¿å­˜ç‰¹å¾åç§°
```

**`prepare_features()` - ç‰¹å¾å‡†å¤‡**
```python
def prepare_features(self, data: pd.DataFrame, feature_columns: Optional[List[str]] = None,
                    label_column: Optional[str] = None, fit_scaler: bool = True) -> Tuple[np.ndarray, Optional[np.ndarray]]:
    # 1. ç¡®å®šç‰¹å¾åˆ—
    if feature_columns is None:
        feature_columns = self.feature_columns or self.feature_names or []
    
    # 2. æ£€æŸ¥ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
    missing_cols = [col for col in feature_columns if col not in data.columns]
    if missing_cols:
        raise ValueError(f"æ•°æ®ä¸­ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}")
    
    # 3. æå–ç‰¹å¾
    X = data[feature_columns].values
    
    # 4. å¤„ç†NaNå€¼
    if np.isnan(X).any():
        X = np.nan_to_num(X, nan=0.0)
    
    # 5. ç‰¹å¾æ ‡å‡†åŒ–
    if fit_scaler or self.scaler is None:
        self.scaler = StandardScaler()
        X = self.scaler.fit_transform(X)  # è®­ç»ƒæ—¶æ‹Ÿåˆ
    else:
        X = self.scaler.transform(X)  # é¢„æµ‹æ—¶è½¬æ¢
    
    # 6. æ ‡ç­¾ç¼–ç 
    y = None
    if label_column and label_column in data.columns:
        if self.label_encoder is None:
            self.label_encoder = LabelEncoder()
            y = self.label_encoder.fit_transform(data[label_column])  # è®­ç»ƒæ—¶æ‹Ÿåˆ
        else:
            y = self.label_encoder.transform(data[label_column])  # é¢„æµ‹æ—¶è½¬æ¢
    
    return X, y
```

**`save_processor()` å’Œ `load_processor()` - çŠ¶æ€æŒä¹…åŒ–**
```python
def save_processor(self, file_path: Union[str, Path]):
    # ä¿å­˜å¤„ç†å™¨çŠ¶æ€
    processor_state = {
        'text_column': self.text_column,
        'feature_names': self.feature_names,
        'scaler': self.scaler,  # ä¿å­˜æ ‡å‡†åŒ–å™¨
        'label_encoder': self.label_encoder  # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
    }
    
    # ä¿å­˜pickleæ ¼å¼
    with open(file_path, 'wb') as f:
        pickle.dump(processor_state, f)
    
    # åŒæ—¶ä¿å­˜JSONæ ¼å¼ï¼ˆè·¨è¯­è¨€å…¼å®¹ï¼‰
    json_state = {
        'scaler_mean': self.scaler.mean_.tolist() if self.scaler else None,
        'scaler_scale': self.scaler.scale_.tolist() if self.scaler else None,
        'label_classes': self.label_encoder.classes_.tolist() if self.label_encoder else None
    }
    # ä¿å­˜JSON...

def load_processor(self, file_path: Union[str, Path]):
    # åŠ è½½å¤„ç†å™¨çŠ¶æ€
    with open(file_path, 'rb') as f:
        processor_state = pickle.load(f)
    
    # æ¢å¤æ‰€æœ‰çŠ¶æ€
    self.text_column = processor_state['text_column']
    self.feature_names = processor_state['feature_names']
    self.scaler = processor_state['scaler']  # æ¢å¤æ ‡å‡†åŒ–å™¨
    self.label_encoder = processor_state['label_encoder']  # æ¢å¤æ ‡ç­¾ç¼–ç å™¨
```

#### è®¾è®¡åŸç†
1. **æµç¨‹æ ‡å‡†åŒ–**ï¼š
   - ç¡®ä¿æ•°æ®å¤„ç†æµç¨‹çš„ä¸€è‡´æ€§
   - é¿å…æ•°æ®æ³„éœ²ï¼ˆè®­ç»ƒå’Œæµ‹è¯•ä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–å™¨ï¼‰
   - ä¿è¯ç‰¹å¾ç»´åº¦çš„ä¸€è‡´æ€§

2. **çŠ¶æ€ç®¡ç†**ï¼š
   - ä¿å­˜å¤„ç†å™¨çš„çŠ¶æ€ï¼Œç¡®ä¿é¢„æµ‹æ—¶ä½¿ç”¨ç›¸åŒçš„å‚æ•°
   - æ”¯æŒå¢é‡è®­ç»ƒå’Œæ¨¡å‹æ›´æ–°
   - æä¾›è·¨è¯­è¨€å…¼å®¹æ€§ï¼ˆJSONæ ¼å¼ï¼‰

3. **é”™è¯¯å¤„ç†**ï¼š
   - å¤„ç†NaNå€¼å’Œå¼‚å¸¸æƒ…å†µ
   - æä¾›è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯
   - ç¡®ä¿æ•°æ®å®Œæ•´æ€§

## ğŸ¯ åŠŸèƒ½è¯¦è§£

### æ–‡æœ¬æ¸…æ´—åŠŸèƒ½

#### æ”¯æŒçš„æ¸…æ´—æ“ä½œ
1. **HTMLè§£ç ** - è§£ç HTMLå®ä½“å­—ç¬¦
2. **Unicodeæ ‡å‡†åŒ–** - ç»Ÿä¸€å­—ç¬¦ç¼–ç 
3. **URLç§»é™¤** - æ›¿æ¢ä¸º `[URL]` æ ‡è®°
4. **é‚®ç®±ç§»é™¤** - æ›¿æ¢ä¸º `[EMAIL]` æ ‡è®°
5. **ç”µè¯å·ç ç§»é™¤** - æ›¿æ¢ä¸º `[PHONE]` æ ‡è®°
6. **è¡¨æƒ…ç¬¦å·æ ‡å‡†åŒ–** - è½¬æ¢ä¸ºè¯­ä¹‰æ ‡ç­¾
7. **é‡å¤å­—ç¬¦å¤„ç†** - é™åˆ¶é‡å¤å­—ç¬¦æ•°é‡
8. **é‡å¤å•è¯å¤„ç†** - ç§»é™¤é‡å¤å•è¯
9. **å¤§å°å†™è½¬æ¢** - ç»Ÿä¸€ä¸ºå°å†™
10. **ç©ºæ ¼æ¸…ç†** - è§„èŒƒåŒ–ç©ºæ ¼

#### è¡¨æƒ…ç¬¦å·æ˜ å°„
```python
emoticon_map = {
    ':)': ' [happy] ', ':-)': ' [happy] ', ':D': ' [happy] ',
    ':(': ' [sad] ', ':-(': ' [sad] ', ':((': ' [sad] ',
    ';)': ' [wink] ', ';-)': ' [wink] ',
    ':P': ' [tongue] ', ':-P': ' [tongue] ',
    '<3': ' [heart] ', '</3': ' [broken_heart] ',
    'T_T': ' [crying] ', 'T.T': ' [crying] ',
    '^_^': ' [smile] ', '^^': ' [smile] ',
    '>:(': ' [angry] ', '>:((': ' [angry] ',
    ':|': ' [neutral] ', ':-|': ' [neutral] '
}
```

### ç‰¹å¾æå–åŠŸèƒ½

#### è¯­è¨€å­¦ç‰¹å¾ï¼ˆ12ä¸ªï¼‰
- `text_length` - æ–‡æœ¬é•¿åº¦
- `word_count` - å•è¯æ•°é‡
- `char_count` - å­—ç¬¦æ•°é‡
- `avg_word_length` - å¹³å‡å•è¯é•¿åº¦
- `sentence_count` - å¥å­æ•°é‡
- `avg_sentence_length` - å¹³å‡å¥å­é•¿åº¦
- `unique_words` - å”¯ä¸€å•è¯æ•°
- `lexical_diversity` - è¯æ±‡å¤šæ ·æ€§
- `type_token_ratio` - ç±»å‹æ ‡è®°æ¯”
- `exclamation_count` - æ„Ÿå¹å·æ•°é‡
- `question_count` - é—®å·æ•°é‡
- `ellipsis_count` - çœç•¥å·æ•°é‡
- `caps_words_count` - å¤§å†™å•è¯æ•°é‡
- `uppercase_ratio` - å¤§å†™å­—æ¯æ¯”ä¾‹

#### æŠ‘éƒç‰¹å¾ï¼ˆ11ä¸ªï¼‰
- `depression_æƒ…ç»ªä½è½_count` - æƒ…ç»ªä½è½ç›¸å…³è¯æ±‡æ•°é‡
- `depression_å…´è¶£ä¸§å¤±_count` - å…´è¶£ä¸§å¤±ç›¸å…³è¯æ±‡æ•°é‡
- `depression_ç¡çœ é—®é¢˜_count` - ç¡çœ é—®é¢˜ç›¸å…³è¯æ±‡æ•°é‡
- `depression_é£Ÿæ¬²å˜åŒ–_count` - é£Ÿæ¬²å˜åŒ–ç›¸å…³è¯æ±‡æ•°é‡
- `depression_æ³¨æ„åŠ›é—®é¢˜_count` - æ³¨æ„åŠ›é—®é¢˜ç›¸å…³è¯æ±‡æ•°é‡
- `depression_è‡ªæˆ‘è¯„ä»·_count` - è‡ªæˆ‘è¯„ä»·ç›¸å…³è¯æ±‡æ•°é‡
- `depression_è‡ªæ€æƒ³æ³•_count` - è‡ªæ€æƒ³æ³•ç›¸å…³è¯æ±‡æ•°é‡
- `depression_ç„¦è™‘ç—‡çŠ¶_count` - ç„¦è™‘ç—‡çŠ¶ç›¸å…³è¯æ±‡æ•°é‡
- `depression_èº«ä½“ç—‡çŠ¶_count` - èº«ä½“ç—‡çŠ¶ç›¸å…³è¯æ±‡æ•°é‡
- `depression_ç¤¾äº¤é€€ç¼©_count` - ç¤¾äº¤é€€ç¼©ç›¸å…³è¯æ±‡æ•°é‡
- `total_depression_words` - æ€»æŠ‘éƒè¯æ±‡æ•°é‡
- `depression_word_density` - æŠ‘éƒè¯æ±‡å¯†åº¦
- `depression_categories` - æŠ‘éƒè¯æ±‡ç±»åˆ«æ•°

#### æƒ…æ„Ÿç‰¹å¾ï¼ˆ6ä¸ªï¼‰
- `positive_emotion_count` - ç§¯ææƒ…æ„Ÿè¯æ±‡æ•°é‡
- `negative_emotion_count` - æ¶ˆææƒ…æ„Ÿè¯æ±‡æ•°é‡
- `neutral_emotion_count` - ä¸­æ€§æƒ…æ„Ÿè¯æ±‡æ•°é‡
- `total_emotion_words` - æ€»æƒ…æ„Ÿè¯æ±‡æ•°é‡
- `emotion_word_density` - æƒ…æ„Ÿè¯æ±‡å¯†åº¦
- `emotion_polarity` - æƒ…æ„Ÿææ€§ï¼ˆ-1åˆ°1ï¼‰

#### ç¤¾äº¤åª’ä½“ç‰¹å¾ï¼ˆ8ä¸ªï¼‰
- `hashtag_count` - è¯é¢˜æ ‡ç­¾æ•°é‡
- `mention_count` - @æåŠæ•°é‡
- `url_count` - URLæ•°é‡
- `emoticon_count` - è¡¨æƒ…ç¬¦å·æ•°é‡
- `repeated_char_count` - é‡å¤å­—ç¬¦æ•°é‡
- `repeated_word_count` - é‡å¤å•è¯æ•°é‡
- `exclamation_density` - æ„Ÿå¹å·å¯†åº¦
- `question_density` - é—®å·å¯†åº¦

### æ•°æ®æ ‡å‡†åŒ–åŠŸèƒ½

#### StandardScaleræ ‡å‡†åŒ–
```python
# æ ‡å‡†åŒ–å…¬å¼ï¼šz = (x - Î¼) / Ïƒ
# å…¶ä¸­ Î¼ æ˜¯å‡å€¼ï¼ŒÏƒ æ˜¯æ ‡å‡†å·®
X_scaled = (X - X.mean()) / X.std()
```

#### LabelEncoderç¼–ç 
```python
# å°†åˆ†ç±»æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
# ä¾‹å¦‚ï¼š['ä½é£é™©', 'é«˜é£é™©'] -> [0, 1]
y_encoded = label_encoder.fit_transform(y)
```

## ğŸ”¬ è®¾è®¡åŸç†

### 1. æ¨¡å—åŒ–è®¾è®¡
- **å•ä¸€èŒè´£**ï¼šæ¯ä¸ªç±»åªè´Ÿè´£ä¸€ä¸ªç‰¹å®šåŠŸèƒ½
- **æ¾è€¦åˆ**ï¼šç»„ä»¶ä¹‹é—´é€šè¿‡æ¥å£äº¤äº’
- **é«˜å†…èš**ï¼šç›¸å…³åŠŸèƒ½é›†ä¸­åœ¨åŒä¸€ä¸ªç±»ä¸­

### 2. å¯æ‰©å±•æ€§è®¾è®¡
- **æ’ä»¶å¼æ¶æ„**ï¼šå¯ä»¥è½»æ¾æ·»åŠ æ–°çš„ç‰¹å¾æå–å™¨
- **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡å‚æ•°æ§åˆ¶å¤„ç†è¡Œä¸º
- **æ¥å£æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€çš„æ¥å£ä¾¿äºæ‰©å±•

### 3. é²æ£’æ€§è®¾è®¡
- **å¼‚å¸¸å¤„ç†**ï¼šå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
- **é»˜è®¤å€¼æœºåˆ¶**ï¼šæä¾›åˆç†çš„é»˜è®¤å€¼
- **æ•°æ®éªŒè¯**ï¼šç¡®ä¿æ•°æ®è´¨é‡

### 4. æ€§èƒ½ä¼˜åŒ–
- **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡æ“ä½œæé«˜æ•ˆç‡
- **æ­£åˆ™è¡¨è¾¾å¼ç¼–è¯‘**ï¼šé¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
- **å†…å­˜ç®¡ç†**ï¼šåˆç†ç®¡ç†å†…å­˜ä½¿ç”¨

## ğŸ’» ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨æµç¨‹

```python
from src.data_processing.preprocessor import DataProcessor

# 1. åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
processor = DataProcessor(
    text_column='text',
    label_column='label',
    min_text_length=10,
    max_text_length=1000
)

# 2. åŠ è½½æ•°æ®
data = processor.load_data('data/raw/sample_data.csv')

# 3. å¤„ç†æ•°æ®
processed_data = processor.process_social_media_data(data)

# 4. å‡†å¤‡ç‰¹å¾
X_train, y_train = processor.prepare_features(
    processed_data, 
    label_column='label', 
    fit_scaler=True
)

# 5. ä¿å­˜å¤„ç†å™¨çŠ¶æ€
processor.save_processor('models/data_processor.pkl')
```

### é¢„æµ‹æ—¶ä½¿ç”¨

```python
# 1. åŠ è½½å¤„ç†å™¨çŠ¶æ€
processor = DataProcessor()
processor.load_processor('models/data_processor.pkl')

# 2. å¤„ç†æ–°æ•°æ®
new_data = pd.DataFrame({'text': ['I feel sad today']})
processed_new_data = processor.process_social_media_data(new_data)

# 3. å‡†å¤‡ç‰¹å¾ï¼ˆä¸æ‹Ÿåˆæ ‡å‡†åŒ–å™¨ï¼‰
X_new, _ = processor.prepare_features(
    processed_new_data, 
    fit_scaler=False
)

# 4. ä½¿ç”¨æ¨¡å‹é¢„æµ‹
prediction = model.predict(X_new)
```

### æ‰¹é‡å¤„ç†ç¤ºä¾‹

```python
# æ‰¹é‡æ¸…æ´—æ–‡æœ¬
texts = [
    "I am so happy today!!! ğŸ˜Š #blessed @friend",
    "I feel so sad and hopeless... :( #depression",
    "RT @user: This is a retweet with http://example.com"
]

cleaner = TextCleaner()
cleaned_texts = cleaner.clean_batch(texts)

# æ‰¹é‡æå–ç‰¹å¾
extractor = FeatureExtractor()
features_list = extractor.extract_batch_features(cleaned_texts)
```

### è‡ªå®šä¹‰ç‰¹å¾åˆ—

```python
# è®¾ç½®è¦ä½¿ç”¨çš„ç‰¹å¾åˆ—
processor.set_feature_columns([
    'text_length', 'word_count', 'avg_word_length',
    'depression_æƒ…ç»ªä½è½_count', 'depression_è‡ªæ€æƒ³æ³•_count',
    'positive_emotion_count', 'negative_emotion_count',
    'emotion_polarity'
])

# ä½¿ç”¨æŒ‡å®šçš„ç‰¹å¾åˆ—
X_train, y_train = processor.prepare_features(
    processed_data, 
    label_column='label', 
    fit_scaler=True
)
```

## ğŸ› ï¸ æœ€ä½³å®è·µ

### 1. æ•°æ®è´¨é‡ä¿è¯
- **æ–‡æœ¬é•¿åº¦è¿‡æ»¤**ï¼šç§»é™¤è¿‡çŸ­æˆ–è¿‡é•¿çš„æ–‡æœ¬
- **ç©ºå€¼å¤„ç†**ï¼šç§»é™¤ç©ºæ–‡æœ¬å’Œç©ºç™½æ–‡æœ¬
- **é‡å¤æ•°æ®æ£€æŸ¥**ï¼šè¯†åˆ«å’Œå¤„ç†é‡å¤æ•°æ®

### 2. ç‰¹å¾å·¥ç¨‹æœ€ä½³å®è·µ
- **ç‰¹å¾é€‰æ‹©**ï¼šé€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾
- **ç‰¹å¾ç¼©æ”¾**ï¼šç¡®ä¿ç‰¹å¾åœ¨ç›¸åŒå°ºåº¦
- **ç‰¹å¾éªŒè¯**ï¼šéªŒè¯ç‰¹å¾çš„è´¨é‡å’Œæœ‰æ•ˆæ€§

### 3. æ¨¡å‹ä¸€è‡´æ€§ä¿è¯
- **çŠ¶æ€ä¿å­˜**ï¼šä¿å­˜å¤„ç†å™¨çš„å®Œæ•´çŠ¶æ€
- **ç‰ˆæœ¬æ§åˆ¶**ï¼šè®°å½•å¤„ç†å™¨çš„ç‰ˆæœ¬ä¿¡æ¯
- **æµ‹è¯•éªŒè¯**ï¼šå®šæœŸæµ‹è¯•å¤„ç†å™¨çš„åŠŸèƒ½

### 4. æ€§èƒ½ä¼˜åŒ–å»ºè®®
- **æ‰¹é‡å¤„ç†**ï¼šä½¿ç”¨æ‰¹é‡æ“ä½œæé«˜æ•ˆç‡
- **å†…å­˜ç®¡ç†**ï¼šåŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„æ•°æ®
- **ç¼“å­˜æœºåˆ¶**ï¼šç¼“å­˜ä¸­é—´ç»“æœé¿å…é‡å¤è®¡ç®—

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ–‡æœ¬æ¸…æ´—ï¼Ÿ
**A:** ç¤¾äº¤åª’ä½“æ–‡æœ¬åŒ…å«å¤§é‡å™ªå£°ä¿¡æ¯ï¼ˆURLã€è¡¨æƒ…ç¬¦å·ã€é‡å¤å­—ç¬¦ç­‰ï¼‰ï¼Œè¿™äº›ä¿¡æ¯ä¼šå¹²æ‰°æœºå™¨å­¦ä¹ æ¨¡å‹çš„å­¦ä¹ ã€‚æ–‡æœ¬æ¸…æ´—å¯ä»¥ï¼š
- ç»Ÿä¸€æ–‡æœ¬æ ¼å¼
- ç§»é™¤æ— å…³ä¿¡æ¯
- ä¿ç•™æœ‰ç”¨ä¿¡æ¯ï¼ˆå¦‚è¡¨æƒ…ç¬¦å·çš„æƒ…æ„Ÿä¿¡æ¯ï¼‰

### Q2: ç‰¹å¾æå–ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›ç‰¹å¾ï¼Ÿ
**A:** è¿™äº›ç‰¹å¾æ˜¯åŸºäºä»¥ä¸‹è€ƒè™‘é€‰æ‹©çš„ï¼š
- **è¯­è¨€å­¦ç‰¹å¾**ï¼šåæ˜ æ–‡æœ¬çš„åŸºæœ¬ç»“æ„å’Œå¤æ‚åº¦
- **æŠ‘éƒç‰¹å¾**ï¼šåŸºäºä¸´åºŠå¿ƒç†å­¦ç†è®ºï¼ˆPHQ-9é—®å·ï¼‰
- **æƒ…æ„Ÿç‰¹å¾**ï¼šæ•æ‰æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘
- **ç¤¾äº¤åª’ä½“ç‰¹å¾**ï¼šé€‚åº”ç¤¾äº¤åª’ä½“æ–‡æœ¬çš„ç‰¹æ®Šæ€§

### Q3: ä¸ºä»€ä¹ˆè¦ä¿å­˜å¤„ç†å™¨çŠ¶æ€ï¼Ÿ
**A:** ä¿å­˜å¤„ç†å™¨çŠ¶æ€æ˜¯ä¸ºäº†ç¡®ä¿ï¼š
- **ä¸€è‡´æ€§**ï¼šè®­ç»ƒå’Œé¢„æµ‹æ—¶ä½¿ç”¨ç›¸åŒçš„å‚æ•°
- **å¯é‡ç°æ€§**ï¼šå¯ä»¥é‡ç°ç›¸åŒçš„å¤„ç†ç»“æœ
- **éƒ¨ç½²ä¾¿åˆ©**ï¼šä¾¿äºæ¨¡å‹éƒ¨ç½²å’Œæ›´æ–°

### Q4: å¦‚ä½•å¤„ç†æ–°å‡ºç°çš„è¯æ±‡ï¼Ÿ
**A:** å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¤„ç†ï¼š
- **æ‰©å±•è¯æ±‡è¡¨**ï¼šå®šæœŸæ›´æ–°å…³é”®è¯åˆ—è¡¨
- **åŠ¨æ€ç‰¹å¾**ï¼šä½¿ç”¨è¯å‘é‡ç­‰åŠ¨æ€ç‰¹å¾
- **è¿ç§»å­¦ä¹ **ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

### Q5: ç‰¹å¾ç»´åº¦ä¸åŒ¹é…æ€ä¹ˆåŠï¼Ÿ
**A:** å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³ï¼š
- **ç‰¹å¾å¯¹é½**ï¼šç¡®ä¿è®­ç»ƒå’Œé¢„æµ‹æ—¶ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾
- **é»˜è®¤å€¼å¡«å……**ï¼šä¸ºç¼ºå¤±ç‰¹å¾æä¾›é»˜è®¤å€¼
- **ç‰¹å¾é€‰æ‹©**ï¼šä½¿ç”¨å›ºå®šçš„ç‰¹å¾å­é›†

## ğŸ“š å‚è€ƒèµ„æ–™

1. **PHQ-9é—®å·**ï¼šæŠ‘éƒç—‡ç­›æŸ¥å·¥å…·
2. **æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯**ï¼šNLPåŸºç¡€æŠ€æœ¯
3. **ç‰¹å¾å·¥ç¨‹æ–¹æ³•**ï¼šæœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹
4. **æ•°æ®æ ‡å‡†åŒ–**ï¼šæœºå™¨å­¦ä¹ æ•°æ®é¢„å¤„ç†
5. **æ¨¡å‹æŒä¹…åŒ–**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹éƒ¨ç½²

---

*æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†æ•°æ®é¢„å¤„ç†æ¨¡å—çš„è®¾è®¡åŸç†ã€åŠŸèƒ½å®ç°å’Œä½¿ç”¨æ–¹æ³•ï¼Œä¸ºé¡¹ç›®çš„å¼€å‘å’Œç»´æŠ¤æä¾›äº†å…¨é¢çš„æŠ€æœ¯æŒ‡å¯¼ã€‚*
