# ğŸ“Š æ•°æ®é¢„å¤„ç†æ¨¡å—è¯¦è§£

## ğŸ¯ æ¨¡å—æ¦‚è¿°

æ•°æ®é¢„å¤„ç†æ¨¡å—æ˜¯æŠ‘éƒé£é™©é¢„è­¦ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†åŸå§‹çš„ç¤¾äº¤åª’ä½“æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥ç†è§£çš„æ ‡å‡†åŒ–ç‰¹å¾ã€‚æ•´ä¸ªæ¨¡å—é‡‡ç”¨**æ¨¡å—åŒ–è®¾è®¡**ï¼Œåˆ†ä¸ºä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

```
æ•°æ®é¢„å¤„ç†æ¨¡å—
â”œâ”€â”€ ğŸ§¹ TextCleaner (æ–‡æœ¬æ¸…æ´—å™¨)
â”œâ”€â”€ ğŸ” FeatureExtractor (ç‰¹å¾æå–å™¨)  
â””â”€â”€ ğŸ›ï¸ DataProcessor (æ•°æ®å¤„ç†å™¨)
```

---

## ğŸ§¹ 1. æ–‡æœ¬æ¸…æ´—å™¨ (TextCleaner)

### ğŸ“‹ ä¸»è¦åŠŸèƒ½
å°†æ··ä¹±çš„ç¤¾äº¤åª’ä½“æ–‡æœ¬è½¬æ¢ä¸ºå¹²å‡€ã€æ ‡å‡†åŒ–çš„æ–‡æœ¬æ•°æ®ã€‚

### ğŸ”§ æ ¸å¿ƒé€»è¾‘

#### 1.1 åˆå§‹åŒ–é˜¶æ®µ
```python
class TextCleaner:
    def __init__(self):
        # è¡¨æƒ…ç¬¦å·æ˜ å°„è¡¨
        self.emoticon_map = {
            ':)': ' [happy] ', ':-)': ' [happy] ',
            ':(': ' [sad] ', ':-(': ' [sad] ',
            # ... æ›´å¤šè¡¨æƒ…ç¬¦å·
        }
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.patterns = {
            'url': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'(\+?1?[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})',
            # ... æ›´å¤šæ¨¡å¼
        }
```

**è§£é‡Š**ï¼š
- **è¡¨æƒ…ç¬¦å·æ˜ å°„**ï¼šå°†å„ç§è¡¨æƒ…ç¬¦å·ç»Ÿä¸€è½¬æ¢ä¸ºæ ‡å‡†æ ‡ç­¾ï¼Œä¾¿äºåç»­åˆ†æ
- **æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼**ï¼šå®šä¹‰å„ç§éœ€è¦å¤„ç†çš„æ–‡æœ¬æ¨¡å¼ï¼ˆURLã€é‚®ç®±ã€ç”µè¯ç­‰ï¼‰

#### 1.2 æ–‡æœ¬æ¸…æ´—æµç¨‹
```python
def clean_text(self, text: str, remove_urls=True, ...) -> str:
    # 1. HTMLè§£ç 
    text = html.unescape(text)
    
    # 2. Unicodeæ ‡å‡†åŒ–
    text = unicodedata.normalize('NFKC', text)
    
    # 3. ç§»é™¤å„ç§æ¨¡å¼
    if remove_urls:
        text = self.compiled_patterns['url'].sub(' [URL] ', text)
    
    # 4. æ ‡å‡†åŒ–è¡¨æƒ…ç¬¦å·
    if normalize_emoticons:
        text = self._normalize_emoticons(text)
    
    # 5. å¤„ç†é‡å¤å†…å®¹
    if remove_repeated_chars:
        text = self.compiled_patterns['repeated_chars'].sub(r'\1\1', text)
    
    # 6. è½¬æ¢ä¸ºå°å†™å¹¶æ¸…ç†ç©ºæ ¼
    if lowercase:
        text = text.lower()
    text = self.compiled_patterns['extra_spaces'].sub(' ', text)
```

**æ¸…æ´—æ­¥éª¤è¯¦è§£**ï¼š

1. **HTMLè§£ç **ï¼šå°† `&amp;` ç­‰HTMLå®ä½“è½¬æ¢ä¸ºæ­£å¸¸å­—ç¬¦
2. **Unicodeæ ‡å‡†åŒ–**ï¼šç»Ÿä¸€å­—ç¬¦ç¼–ç æ ¼å¼
3. **æ¨¡å¼æ›¿æ¢**ï¼šå°†URLã€é‚®ç®±ç­‰æ›¿æ¢ä¸ºæ ‡å‡†åŒ–æ ‡ç­¾
4. **è¡¨æƒ…ç¬¦å·æ ‡å‡†åŒ–**ï¼šå°† `:)` è½¬æ¢ä¸º `[happy]`
5. **é‡å¤å†…å®¹å¤„ç†**ï¼šå°† `loooool` è½¬æ¢ä¸º `lool`
6. **æ ¼å¼æ ‡å‡†åŒ–**ï¼šè½¬æ¢ä¸ºå°å†™ï¼Œæ¸…ç†å¤šä½™ç©ºæ ¼

#### 1.3 ç‰¹å¾æå–
```python
def extract_features(self, text: str) -> Dict[str, any]:
    features = {
        'text_length': len(text),
        'word_count': len(text.split()),
        'has_url': bool(self.compiled_patterns['url'].search(text)),
        'has_email': bool(self.compiled_patterns['email'].search(text)),
        'emoticon_count': 0,
        # ... æ›´å¤šç‰¹å¾
    }
```

**æå–çš„ç‰¹å¾**ï¼š
- **åŸºæœ¬ç»Ÿè®¡**ï¼šæ–‡æœ¬é•¿åº¦ã€å•è¯æ•°é‡ã€å­—ç¬¦æ•°é‡
- **å†…å®¹æ£€æµ‹**ï¼šæ˜¯å¦åŒ…å«URLã€é‚®ç®±ã€ç”µè¯ç­‰
- **ç‰¹æ®Šå†…å®¹**ï¼šè¡¨æƒ…ç¬¦å·æ•°é‡ã€é‡å¤å­—ç¬¦/å•è¯æ•°é‡

---

## ğŸ” 2. ç‰¹å¾æå–å™¨ (FeatureExtractor)

### ğŸ“‹ ä¸»è¦åŠŸèƒ½
ä»æ¸…æ´—åçš„æ–‡æœ¬ä¸­æå–æœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹æä¾›è¾“å…¥ã€‚

### ğŸ”§ æ ¸å¿ƒé€»è¾‘

#### 2.1 æŠ‘éƒç›¸å…³è¯æ±‡åº“
```python
self.depression_keywords = {
    'æƒ…ç»ªä½è½': ['sad', 'depressed', 'down', 'blue', 'unhappy', 'miserable', 'hopeless'],
    'å…´è¶£ä¸§å¤±': ['uninterested', 'bored', 'no_interest', 'nothing_matters', 'empty'],
    'ç¡çœ é—®é¢˜': ['insomnia', 'sleep', 'tired', 'exhausted', 'fatigue', 'restless'],
    'é£Ÿæ¬²å˜åŒ–': ['appetite', 'hungry', 'not_hungry', 'weight_loss', 'weight_gain'],
    'æ³¨æ„åŠ›é—®é¢˜': ['concentrate', 'focus', 'attention', 'distracted', 'mind_wandering'],
    'è‡ªæˆ‘è¯„ä»·': ['worthless', 'failure', 'useless', 'guilty', 'blame_myself'],
    'è‡ªæ€æƒ³æ³•': ['suicide', 'kill_myself', 'death', 'die', 'end_it_all', 'better_off_dead'],
    'ç„¦è™‘ç—‡çŠ¶': ['anxious', 'worried', 'nervous', 'panic', 'fear', 'stress'],
    'èº«ä½“ç—‡çŠ¶': ['pain', 'ache', 'sick', 'ill', 'headache', 'stomach'],
    'ç¤¾äº¤é€€ç¼©': ['alone', 'lonely', 'isolated', 'no_friends', 'avoid_people']
}
```

**è§£é‡Š**ï¼šåŸºäºPHQ-9æŠ‘éƒç­›æŸ¥é—®å·ï¼Œå®šä¹‰äº†10ä¸ªæŠ‘éƒç—‡çŠ¶ç±»åˆ«çš„å…³é”®è¯ã€‚

#### 2.2 è¯­è¨€å­¦ç‰¹å¾æå–
```python
def extract_linguistic_features(self, text: str) -> Dict[str, float]:
    features = {}
    
    # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
    features['text_length'] = len(text)
    features['word_count'] = len(text.split())
    features['sentence_count'] = len(re.split(r'[.!?]+', text))
    
    # è¯æ±‡å¤šæ ·æ€§
    words = text.lower().split()
    if words:
        features['unique_words'] = len(set(words))
        features['lexical_diversity'] = features['unique_words'] / features['word_count']
    
    # æ ‡ç‚¹ç¬¦å·ç‰¹å¾
    features['exclamation_count'] = len(self.compiled_patterns['exclamation'].findall(text))
    features['question_count'] = len(self.compiled_patterns['question'].findall(text))
    features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0
```

**è¯­è¨€å­¦ç‰¹å¾åŒ…æ‹¬**ï¼š
- **æ–‡æœ¬ç»Ÿè®¡**ï¼šé•¿åº¦ã€å•è¯æ•°ã€å¥å­æ•°ã€å¹³å‡é•¿åº¦
- **è¯æ±‡å¤šæ ·æ€§**ï¼šç‹¬ç‰¹å•è¯æ•°ã€è¯æ±‡å¤šæ ·æ€§æ¯”ç‡
- **æ ‡ç‚¹ç¬¦å·**ï¼šæ„Ÿå¹å·ã€é—®å·ã€çœç•¥å·æ•°é‡
- **æ ¼å¼ç‰¹å¾**ï¼šå¤§å†™å­—æ¯æ¯”ä¾‹ã€å…¨å¤§å†™å•è¯æ•°

#### 2.3 æŠ‘éƒç‰¹å¾æå–
```python
def extract_depression_features(self, text: str) -> Dict[str, float]:
    features = {}
    text_lower = text.lower()
    
    # ç»Ÿè®¡å„ç±»æŠ‘éƒå…³é”®è¯
    for category, keywords in self.depression_keywords.items():
        count = sum(text_lower.count(keyword) for keyword in keywords)
        features[f'{category}_count'] = count
        features[f'{category}_density'] = count / len(text_lower.split()) if text_lower.split() else 0
    
    # è®¡ç®—æ€»ä½“æŠ‘éƒæŒ‡æ ‡
    total_depression_words = sum(features[f'{cat}_count'] for cat in self.depression_keywords.keys())
    features['total_depression_count'] = total_depression_words
    features['depression_density'] = total_depression_words / len(text_lower.split()) if text_lower.split() else 0
```

**æŠ‘éƒç‰¹å¾åŒ…æ‹¬**ï¼š
- **åˆ†ç±»ç»Ÿè®¡**ï¼šæ¯ä¸ªæŠ‘éƒç—‡çŠ¶ç±»åˆ«çš„å…³é”®è¯æ•°é‡
- **å¯†åº¦æŒ‡æ ‡**ï¼šå…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„å¯†åº¦
- **æ€»ä½“æŒ‡æ ‡**ï¼šæ€»æŠ‘éƒè¯æ±‡æ•°é‡å’Œå¯†åº¦

#### 2.4 æƒ…æ„Ÿç‰¹å¾æå–
```python
def extract_emotion_features(self, text: str) -> Dict[str, float]:
    features = {}
    text_lower = text.lower()
    
    # ç»Ÿè®¡å„ç±»æƒ…æ„Ÿè¯æ±‡
    for emotion, words in self.emotion_words.items():
        count = sum(text_lower.count(word) for word in words)
        features[f'{emotion}_count'] = count
        features[f'{emotion}_density'] = count / len(text_lower.split()) if text_lower.split() else 0
    
    # è®¡ç®—æƒ…æ„Ÿææ€§
    positive_count = features['positive_count']
    negative_count = features['negative_count']
    total_words = len(text_lower.split())
    
    if total_words > 0:
        features['emotion_polarity'] = (positive_count - negative_count) / total_words
        features['emotion_intensity'] = (positive_count + negative_count) / total_words
    else:
        features['emotion_polarity'] = 0
        features['emotion_intensity'] = 0
```

**æƒ…æ„Ÿç‰¹å¾åŒ…æ‹¬**ï¼š
- **æƒ…æ„Ÿç»Ÿè®¡**ï¼šç§¯æã€æ¶ˆæã€ä¸­æ€§è¯æ±‡æ•°é‡
- **æƒ…æ„Ÿå¯†åº¦**ï¼šå„ç±»æƒ…æ„Ÿè¯æ±‡çš„å¯†åº¦
- **æƒ…æ„Ÿææ€§**ï¼šç§¯æ-æ¶ˆæè¯æ±‡çš„å·®å€¼
- **æƒ…æ„Ÿå¼ºåº¦**ï¼šç§¯æ+æ¶ˆæè¯æ±‡çš„æ€»å¯†åº¦

#### 2.5 ç¤¾äº¤åª’ä½“ç‰¹å¾æå–
```python
def extract_social_media_features(self, text: str) -> Dict[str, float]:
    features = {}
    
    # ç¤¾äº¤åª’ä½“ç‰¹å®šæ¨¡å¼
    features['hashtag_count'] = len(re.findall(r'#\w+', text))
    features['mention_count'] = len(re.findall(r'@\w+', text))
    features['url_count'] = len(re.findall(r'http[s]?://\S+', text))
    features['emoticon_count'] = len(re.findall(r'[:;=]-?[)(/|\\pPoO]', text))
    
    # é‡å¤å†…å®¹
    features['repeated_char_count'] = len(re.findall(r'(.)\1{2,}', text))
    features['repeated_word_count'] = len(re.findall(r'\b(\w+)(\s+\1){2,}\b', text))
    
    # æ ‡ç‚¹ç¬¦å·å¯†åº¦
    total_words = len(text.split())
    if total_words > 0:
        features['exclamation_density'] = features['exclamation_count'] / total_words
        features['question_density'] = features['question_count'] / total_words
```

**ç¤¾äº¤åª’ä½“ç‰¹å¾åŒ…æ‹¬**ï¼š
- **å¹³å°ç‰¹å¾**ï¼šè¯é¢˜æ ‡ç­¾ã€@æåŠã€URLã€è¡¨æƒ…ç¬¦å·æ•°é‡
- **é‡å¤å†…å®¹**ï¼šé‡å¤å­—ç¬¦ã€é‡å¤å•è¯æ•°é‡
- **è¡¨è¾¾æ–¹å¼**ï¼šæ„Ÿå¹å·ã€é—®å·å¯†åº¦

---

## ğŸ›ï¸ 3. æ•°æ®å¤„ç†å™¨ (DataProcessor)

### ğŸ“‹ ä¸»è¦åŠŸèƒ½
æ•´åˆæ–‡æœ¬æ¸…æ´—å’Œç‰¹å¾æå–ï¼Œå¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹å‡†å¤‡è®­ç»ƒæ•°æ®ã€‚

### ğŸ”§ æ ¸å¿ƒé€»è¾‘

#### 3.1 æ•°æ®åŠ è½½
```python
def load_data(self, file_path: str, file_type: str = 'auto') -> pd.DataFrame:
    """åŠ è½½æ•°æ®æ–‡ä»¶"""
    if file_type == 'auto':
        file_type = Path(file_path).suffix.lower()
    
    if file_type == '.csv':
        data = pd.read_csv(file_path)
    elif file_type == '.json':
        data = pd.read_json(file_path)
    elif file_type == '.txt':
        data = pd.read_csv(file_path, sep='\t')
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
    
    return data
```

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼**ï¼š
- **CSVæ–‡ä»¶**ï¼šé€—å·åˆ†éš”å€¼
- **JSONæ–‡ä»¶**ï¼šJavaScriptå¯¹è±¡è¡¨ç¤ºæ³•
- **TXTæ–‡ä»¶**ï¼šåˆ¶è¡¨ç¬¦åˆ†éš”çš„æ–‡æœ¬æ–‡ä»¶

#### 3.2 æ•°æ®è¿‡æ»¤
```python
def filter_data(self, data: pd.DataFrame) -> pd.DataFrame:
    """è¿‡æ»¤æ•°æ®"""
    # ç§»é™¤ç©ºå€¼
    data = data.dropna(subset=[self.text_column])
    
    # æŒ‰æ–‡æœ¬é•¿åº¦è¿‡æ»¤
    data = data[
        (data[self.text_column].str.len() >= self.min_text_length) &
        (data[self.text_column].str.len() <= self.max_text_length)
    ]
    
    return data.reset_index(drop=True)
```

**è¿‡æ»¤æ¡ä»¶**ï¼š
- **ç©ºå€¼å¤„ç†**ï¼šç§»é™¤æ–‡æœ¬ä¸ºç©ºçš„è¡Œ
- **é•¿åº¦è¿‡æ»¤**ï¼šåªä¿ç•™æŒ‡å®šé•¿åº¦èŒƒå›´å†…çš„æ–‡æœ¬

#### 3.3 ç‰¹å¾æå–å’Œæ ‡å‡†åŒ–
```python
def prepare_features(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    """å‡†å¤‡ç‰¹å¾æ•°æ®"""
    # æ‰¹é‡æå–ç‰¹å¾
    feature_dicts = self.extract_batch_features(data[self.text_column].tolist())
    
    # è½¬æ¢ä¸ºæ•°ç»„
    feature_array = self.features_to_array(feature_dicts)
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    if self.scaler is None:
        self.scaler = StandardScaler()
        feature_array = self.scaler.fit_transform(feature_array)
    else:
        feature_array = self.scaler.transform(feature_array)
    
    # æ ‡ç­¾ç¼–ç 
    if self.label_encoder is None:
        self.label_encoder = LabelEncoder()
        labels = self.label_encoder.fit_transform(data[self.label_column])
    else:
        labels = self.label_encoder.transform(data[self.label_column])
    
    return feature_array, labels
```

**å¤„ç†æ­¥éª¤**ï¼š
1. **ç‰¹å¾æå–**ï¼šå¯¹æ¯ä¸ªæ–‡æœ¬æå–æ‰€æœ‰ç‰¹å¾
2. **æ•°ç»„è½¬æ¢**ï¼šå°†ç‰¹å¾å­—å…¸è½¬æ¢ä¸ºæ•°å€¼æ•°ç»„
3. **ç‰¹å¾æ ‡å‡†åŒ–**ï¼šä½¿ç”¨StandardScaleræ ‡å‡†åŒ–ç‰¹å¾
4. **æ ‡ç­¾ç¼–ç **ï¼šä½¿ç”¨LabelEncoderç¼–ç æ ‡ç­¾

#### 3.4 æ•°æ®åˆ†å‰²
```python
def split_data(self, features: np.ndarray, labels: np.ndarray, 
               test_size: float = 0.2, random_state: int = 42) -> Tuple:
    """åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
    return train_test_split(
        features, labels, 
        test_size=test_size, 
        random_state=random_state,
        stratify=labels  # ä¿æŒæ ‡ç­¾åˆ†å¸ƒ
    )
```

**åˆ†å‰²ç­–ç•¥**ï¼š
- **æ¯”ä¾‹åˆ†å‰²**ï¼šé»˜è®¤80%è®­ç»ƒï¼Œ20%æµ‹è¯•
- **åˆ†å±‚é‡‡æ ·**ï¼šä¿æŒå„ç±»åˆ«åœ¨è®­ç»ƒå’Œæµ‹è¯•é›†ä¸­çš„æ¯”ä¾‹ä¸€è‡´

#### 3.5 çŠ¶æ€ä¿å­˜å’ŒåŠ è½½
```python
def save_processor(self, file_path: str):
    """ä¿å­˜å¤„ç†å™¨çŠ¶æ€"""
    processor_state = {
        'scaler': self.scaler,
        'label_encoder': self.label_encoder,
        'feature_names': self.feature_names,
        'text_column': self.text_column,
        'label_column': self.label_column,
        'min_text_length': self.min_text_length,
        'max_text_length': self.max_text_length
    }
    
    with open(file_path, 'wb') as f:
        pickle.dump(processor_state, f)

def load_processor(self, file_path: str):
    """åŠ è½½å¤„ç†å™¨çŠ¶æ€"""
    with open(file_path, 'rb') as f:
        processor_state = pickle.load(f)
    
    # æ¢å¤çŠ¶æ€
    for key, value in processor_state.items():
        setattr(self, key, value)
```

**çŠ¶æ€ç®¡ç†**ï¼š
- **ä¿å­˜çŠ¶æ€**ï¼šå°†æ ‡å‡†åŒ–å™¨ã€ç¼–ç å™¨ç­‰ä¿å­˜åˆ°æ–‡ä»¶
- **åŠ è½½çŠ¶æ€**ï¼šä»æ–‡ä»¶æ¢å¤å¤„ç†å™¨çŠ¶æ€ï¼Œç¡®ä¿ä¸€è‡´æ€§

---

## ğŸ”„ å®Œæ•´å¤„ç†æµç¨‹

### ğŸ“Š æ•°æ®æµç¨‹å›¾
```
åŸå§‹æ•°æ® â†’ æ•°æ®åŠ è½½ â†’ æ•°æ®è¿‡æ»¤ â†’ æ–‡æœ¬æ¸…æ´— â†’ ç‰¹å¾æå– â†’ ç‰¹å¾æ ‡å‡†åŒ– â†’ æ•°æ®åˆ†å‰² â†’ è®­ç»ƒæ•°æ®
```

### ğŸ’» ä½¿ç”¨ç¤ºä¾‹
```python
# 1. åˆ›å»ºæ•°æ®å¤„ç†å™¨
processor = DataProcessor(
    text_column='text',
    label_column='label',
    min_text_length=10,
    max_text_length=1000
)

# 2. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
data = processor.load_data('data/raw/sample_data.csv')
data = processor.filter_data(data)

# 3. å‡†å¤‡ç‰¹å¾
features, labels = processor.prepare_features(data)

# 4. åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = processor.split_data(features, labels)

# 5. ä¿å­˜å¤„ç†å™¨çŠ¶æ€
processor.save_processor('models/preprocessor.pkl')
```

---

## ğŸ¯ ç‰¹å¾æ€»ç»“

### ğŸ“ˆ æå–çš„ç‰¹å¾ç±»å‹

| ç‰¹å¾ç±»å‹ | ç‰¹å¾æ•°é‡ | è¯´æ˜ |
|---------|---------|------|
| **è¯­è¨€å­¦ç‰¹å¾** | 12ä¸ª | æ–‡æœ¬é•¿åº¦ã€è¯æ±‡å¤šæ ·æ€§ã€æ ‡ç‚¹ç¬¦å·ç­‰ |
| **æŠ‘éƒç‰¹å¾** | 20ä¸ª | 10ä¸ªç—‡çŠ¶ç±»åˆ« Ã— 2ä¸ªæŒ‡æ ‡ï¼ˆæ•°é‡+å¯†åº¦ï¼‰ |
| **æƒ…æ„Ÿç‰¹å¾** | 8ä¸ª | ç§¯æ/æ¶ˆæ/ä¸­æ€§è¯æ±‡ç»Ÿè®¡å’Œææ€§ |
| **ç¤¾äº¤åª’ä½“ç‰¹å¾** | 8ä¸ª | è¯é¢˜æ ‡ç­¾ã€@æåŠã€é‡å¤å†…å®¹ç­‰ |
| **æ€»è®¡** | **48ä¸ªç‰¹å¾** | ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹æä¾›ä¸°å¯Œçš„è¾“å…¥ |

### ğŸ” å…³é”®ç‰¹å¾ç¤ºä¾‹

**æŠ‘éƒé£é™©æŒ‡æ ‡**ï¼š
- `total_depression_count`ï¼šæ€»æŠ‘éƒè¯æ±‡æ•°é‡
- `depression_density`ï¼šæŠ‘éƒè¯æ±‡å¯†åº¦
- `suicide_count`ï¼šè‡ªæ€ç›¸å…³è¯æ±‡æ•°é‡

**æƒ…æ„ŸçŠ¶æ€æŒ‡æ ‡**ï¼š
- `emotion_polarity`ï¼šæƒ…æ„Ÿææ€§ï¼ˆæ­£å€¼=ç§¯æï¼Œè´Ÿå€¼=æ¶ˆæï¼‰
- `emotion_intensity`ï¼šæƒ…æ„Ÿå¼ºåº¦
- `negative_density`ï¼šæ¶ˆæè¯æ±‡å¯†åº¦

**è¡¨è¾¾æ–¹å¼æŒ‡æ ‡**ï¼š
- `exclamation_density`ï¼šæ„Ÿå¹å·å¯†åº¦
- `repeated_char_count`ï¼šé‡å¤å­—ç¬¦æ•°é‡
- `uppercase_ratio`ï¼šå¤§å†™å­—æ¯æ¯”ä¾‹

---

## ğŸš€ æ¨¡å—ä¼˜åŠ¿

### âœ… è®¾è®¡ä¼˜åŠ¿
1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶ç‹¬ç«‹ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•
2. **å¯é…ç½®æ€§**ï¼šæ”¯æŒå¤šç§æ¸…æ´—å’Œæå–é€‰é¡¹
3. **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†
4. **çŠ¶æ€ç®¡ç†**ï¼šå¯ä¿å­˜å’Œæ¢å¤å¤„ç†çŠ¶æ€

### âœ… åŠŸèƒ½ä¼˜åŠ¿
1. **ä¸“ä¸šæ€§å¼º**ï¼šåŸºäºPHQ-9é—®å·çš„æŠ‘éƒç‰¹å¾
2. **å…¨é¢æ€§**ï¼šæ¶µç›–è¯­è¨€å­¦ã€æƒ…æ„Ÿã€ç¤¾äº¤åª’ä½“ç‰¹å¾
3. **æ ‡å‡†åŒ–**ï¼šè‡ªåŠ¨ç‰¹å¾æ ‡å‡†åŒ–å’Œæ ‡ç­¾ç¼–ç 
4. **å¯æ‰©å±•**ï¼šæ˜“äºæ·»åŠ æ–°çš„ç‰¹å¾ç±»å‹

### âœ… å®ç”¨ä¼˜åŠ¿
1. **æ˜“ç”¨æ€§**ï¼šç®€å•çš„APIæ¥å£
2. **å¥å£®æ€§**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†
3. **æ•ˆç‡æ€§**ï¼šä¼˜åŒ–çš„æ‰¹é‡å¤„ç†ç®—æ³•
4. **å…¼å®¹æ€§**ï¼šæ”¯æŒå¤šç§æ•°æ®æ ¼å¼

---

## ğŸ“ æ€»ç»“

æ•°æ®é¢„å¤„ç†æ¨¡å—æ˜¯æ•´ä¸ªæŠ‘éƒé£é™©é¢„è­¦ç³»ç»Ÿçš„åŸºç¡€ï¼Œå®ƒå°†åŸå§‹çš„ç¤¾äº¤åª’ä½“æ–‡æœ¬è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨çš„æ ‡å‡†åŒ–ç‰¹å¾ã€‚é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶çš„ååŒå·¥ä½œï¼Œå®ç°äº†ä»æ•°æ®æ¸…æ´—åˆ°ç‰¹å¾æå–çš„å®Œæ•´æµç¨‹ï¼Œä¸ºåç»­çš„æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹æä¾›äº†é«˜è´¨é‡çš„æ•°æ®åŸºç¡€ã€‚

**æ ¸å¿ƒä»·å€¼**ï¼š
- ğŸ¯ **ç²¾å‡†è¯†åˆ«**ï¼šåŸºäºä¸“ä¸šåŒ»å­¦æ ‡å‡†çš„æŠ‘éƒç‰¹å¾æå–
- ğŸ”„ **è‡ªåŠ¨åŒ–å¤„ç†**ï¼šä»åŸå§‹æ•°æ®åˆ°è®­ç»ƒæ•°æ®çš„å…¨è‡ªåŠ¨è½¬æ¢
- ğŸ“Š **ä¸°å¯Œç‰¹å¾**ï¼š48ä¸ªå¤šç»´ç‰¹å¾å…¨é¢æè¿°ç”¨æˆ·çŠ¶æ€
- ğŸ›¡ï¸ **è´¨é‡ä¿è¯**ï¼šå¤šé‡æ¸…æ´—å’ŒéªŒè¯ç¡®ä¿æ•°æ®è´¨é‡
