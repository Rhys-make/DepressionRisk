#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾æå–å™¨
ä»æ¸…æ´—åçš„æ–‡æœ¬ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾
"""

import re
import numpy as np
from typing import List, Dict, Tuple, Optional
from collections import Counter
import logging

logger = logging.getLogger(__name__)

class FeatureExtractor:
    """æ–‡æœ¬ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç‰¹å¾æå–å™¨"""
        # æŠ‘éƒç›¸å…³è¯æ±‡ï¼ˆåŸºäºPHQ-9é—®å·ï¼‰
        self.depression_keywords = {
            'æƒ…ç»ªä½è½': ['sad', 'depressed', 'down', 'blue', 'unhappy', 'miserable', 'hopeless', 'sadness', 'depression'],
            'å…´è¶£ä¸§å¤±': ['uninterested', 'bored', 'no_interest', 'nothing_matters', 'empty', 'meaningless', 'pointless'],
            'ç¡çœ é—®é¢˜': ['insomnia', 'sleep', 'tired', 'exhausted', 'fatigue', 'restless', 'cant_sleep', 'sleepless'],
            'é£Ÿæ¬²å˜åŒ–': ['appetite', 'hungry', 'not_hungry', 'weight_loss', 'weight_gain', 'eating'],
            'æ³¨æ„åŠ›é—®é¢˜': ['concentrate', 'focus', 'attention', 'distracted', 'mind_wandering', 'racing_thoughts'],
            'è‡ªæˆ‘è¯„ä»·': ['worthless', 'failure', 'useless', 'guilty', 'blame_myself', 'hate_myself', 'disappointment'],
            'è‡ªæ€æƒ³æ³•': ['suicide', 'kill_myself', 'death', 'die', 'end_it_all', 'better_off_dead', 'want_to_die', 'end_my_life'],
            'ç„¦è™‘ç—‡çŠ¶': ['anxious', 'worried', 'nervous', 'panic', 'fear', 'stress', 'anxiety'],
            'èº«ä½“ç—‡çŠ¶': ['pain', 'ache', 'sick', 'ill', 'headache', 'stomach', 'hurting'],
            'ç¤¾äº¤é€€ç¼©': ['alone', 'lonely', 'isolated', 'no_friends', 'avoid_people', 'loneliness']
        }
        
        # æƒ…æ„Ÿè¯æ±‡ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.emotion_words = {
            'positive': ['happy', 'joy', 'excited', 'love', 'great', 'wonderful', 'amazing', 'fantastic', 'blessed', 'grateful', 'optimistic', 'good', 'nice', 'perfect', 'awesome', 'brilliant', 'excellent', 'fixed', 'solved', 'better', 'relief', 'cheered', 'helped'],
            'negative': ['sad', 'angry', 'frustrated', 'disappointed', 'hate', 'terrible', 'awful', 'horrible', 'hopeless', 'worthless', 'miserable', 'stressful', 'stress', 'anxiety', 'worried', 'scared', 'afraid', 'tired', 'exhausted', 'lonely', 'alone', 'useless', 'failure'],
            'neutral': ['okay', 'fine', 'normal', 'average', 'usual', 'regular']
        }
        
        # è½¬æŠ˜è¯ï¼ˆæ–°å¢ï¼‰
        self.turnaround_words = {
            'ä½†æ˜¯', 'ä¸è¿‡', 'ç„¶è€Œ', 'å¯æ˜¯', 'åªæ˜¯', 'ä¸è¿‡', 'ä½†', 'yet', 'but', 'however',
            'although', 'though', 'even though', 'despite', 'in spite of', 'while',
            'è™½ç„¶', 'å°½ç®¡', 'å³ä½¿', 'å°±ç®—', 'å“ªæ€•', 'çºµç„¶', 'è™½è¯´', 'è™½è¯´å¦‚æ­¤',
            'lol', 'haha', 'ğŸ˜…', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜„', 'ğŸ˜‚', 'ğŸ˜†', 'ğŸ˜‰', 'ğŸ˜'
        }
        
        # æƒ…æ„Ÿè½¬æŠ˜æ¨¡å¼ï¼ˆæ–°å¢ï¼‰
        self.turnaround_patterns = [
            r'ä½†æ˜¯.*?(å¼€å¿ƒ|å¿«ä¹|é«˜å…´|å¥½|æ£’|èµ|çˆ½|èˆ’æœ|è½»æ¾|æ”¾æ¾|è§£å†³|ä¿®å¤|æ²»æ„ˆ|fixed|solved|better|relief)',
            r'ä¸è¿‡.*?(å¼€å¿ƒ|å¿«ä¹|é«˜å…´|å¥½|æ£’|èµ|çˆ½|èˆ’æœ|è½»æ¾|æ”¾æ¾|è§£å†³|ä¿®å¤|æ²»æ„ˆ|fixed|solved|better|relief)',
            r'ç„¶è€Œ.*?(å¼€å¿ƒ|å¿«ä¹|é«˜å…´|å¥½|æ£’|èµ|çˆ½|èˆ’æœ|è½»æ¾|æ”¾æ¾|è§£å†³|ä¿®å¤|æ²»æ„ˆ|fixed|solved|better|relief)',
            r'but.*?(happy|good|great|amazing|wonderful|fantastic|fixed|solved|better|relief|cheered|helped)',
            r'however.*?(happy|good|great|amazing|wonderful|fantastic|fixed|solved|better|relief|cheered|helped)',
            r'yet.*?(happy|good|great|amazing|wonderful|fantastic|fixed|solved|better|relief|cheered|helped)',
            r'ğŸ˜©.*?(ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜¢.*?(ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜­.*?(ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜”.*?(ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜.*?(ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜¤.*?(ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜«.*?(ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜©.*?(lol|haha|ğŸ˜…|ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜¢.*?(lol|haha|ğŸ˜…|ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜­.*?(lol|haha|ğŸ˜…|ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜”.*?(lol|haha|ğŸ˜…|ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜.*?(lol|haha|ğŸ˜…|ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜¤.*?(lol|haha|ğŸ˜…|ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)',
            r'ğŸ˜«.*?(lol|haha|ğŸ˜…|ğŸ˜Š|ğŸ˜‹|ğŸ˜„|ğŸ˜‚|ğŸ˜†|ğŸ˜‰|ğŸ˜)'
        ]
        
        # æ ‡ç‚¹ç¬¦å·æ¨¡å¼
        self.punctuation_patterns = {
            'exclamation': r'!+',
            'question': r'\?+',
            'ellipsis': r'\.{3,}',
            'caps_words': r'\b[A-Z]{2,}\b'
        }
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.compiled_patterns = {k: re.compile(v) for k, v in self.punctuation_patterns.items()}
        self.compiled_turnaround_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.turnaround_patterns]
    
    def extract_linguistic_features(self, text: str) -> Dict[str, float]:
        """
        æå–è¯­è¨€å­¦ç‰¹å¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            è¯­è¨€å­¦ç‰¹å¾å­—å…¸
        """
        features = {}
        
        # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
        features['text_length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len(text)
        features['avg_word_length'] = np.mean([len(word) for word in text.split()]) if text.split() else 0
        features['sentence_count'] = len(re.split(r'[.!?]+', text))
        features['avg_sentence_length'] = features['word_count'] / features['sentence_count'] if features['sentence_count'] > 0 else 0
        
        # è¯æ±‡å¤šæ ·æ€§
        words = text.lower().split()
        if words:
            features['unique_words'] = len(set(words))
            features['lexical_diversity'] = features['unique_words'] / features['word_count']
            features['type_token_ratio'] = features['unique_words'] / features['word_count']
        else:
            features['unique_words'] = 0
            features['lexical_diversity'] = 0
            features['type_token_ratio'] = 0
        
        # æ ‡ç‚¹ç¬¦å·ç‰¹å¾
        features['exclamation_count'] = len(self.compiled_patterns['exclamation'].findall(text))
        features['question_count'] = len(self.compiled_patterns['question'].findall(text))
        features['ellipsis_count'] = len(self.compiled_patterns['ellipsis'].findall(text))
        features['caps_words_count'] = len(self.compiled_patterns['caps_words'].findall(text))
        
        # å¤§å†™å­—æ¯æ¯”ä¾‹
        features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0
        
        return features
    
    def extract_depression_features(self, text: str) -> Dict[str, float]:
        """
        æå–æŠ‘éƒç›¸å…³ç‰¹å¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æŠ‘éƒç›¸å…³ç‰¹å¾å­—å…¸
        """
        features = {}
        text_lower = text.lower()
        
        # è®¡ç®—å„ç±»æŠ‘éƒå…³é”®è¯çš„å‡ºç°æ¬¡æ•°ï¼ˆæ”¹è¿›åŒ¹é…æ–¹æ³•ï¼‰
        for category, keywords in self.depression_keywords.items():
            count = 0
            for keyword in keywords:
                # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
                pattern = r'\b' + re.escape(keyword) + r'\b'
                count += len(re.findall(pattern, text_lower))
            features[f'depression_{category}_count'] = count
        
        # æ€»æŠ‘éƒå…³é”®è¯æ•°é‡
        all_depression_words = [word for words in self.depression_keywords.values() for word in words]
        features['total_depression_words'] = sum(text_lower.count(word) for word in all_depression_words)
        
        # æŠ‘éƒè¯æ±‡å¯†åº¦
        word_count = len(text.split())
        features['depression_word_density'] = features['total_depression_words'] / word_count if word_count > 0 else 0
        
        # æŠ‘éƒè¯æ±‡ç±»åˆ«æ•°
        features['depression_categories'] = sum(1 for category, keywords in self.depression_keywords.items() 
                                              if any(text_lower.count(keyword) > 0 for keyword in keywords))
        
        return features
    
    def extract_emotion_features(self, text: str) -> Dict[str, float]:
        """
        æå–æƒ…æ„Ÿç‰¹å¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æƒ…æ„Ÿç‰¹å¾å­—å…¸
        """
        features = {}
        text_lower = text.lower()
        
        # è®¡ç®—å„ç±»æƒ…æ„Ÿè¯æ±‡çš„å‡ºç°æ¬¡æ•°
        for emotion, words in self.emotion_words.items():
            count = sum(text_lower.count(word) for word in words)
            features[f'{emotion}_emotion_count'] = count
        
        # æƒ…æ„Ÿè¯æ±‡æ€»æ•°
        all_emotion_words = [word for words in self.emotion_words.values() for word in words]
        features['total_emotion_words'] = sum(text_lower.count(word) for word in all_emotion_words)
        
        # æƒ…æ„Ÿè¯æ±‡å¯†åº¦
        word_count = len(text.split())
        features['emotion_word_density'] = features['total_emotion_words'] / word_count if word_count > 0 else 0
        
        # æƒ…æ„Ÿææ€§
        positive_count = features['positive_emotion_count']
        negative_count = features['negative_emotion_count']
        total_emotion = positive_count + negative_count
        
        if total_emotion > 0:
            features['emotion_polarity'] = (positive_count - negative_count) / total_emotion
        else:
            features['emotion_polarity'] = 0
        
        return features
    
    def extract_social_media_features(self, text: str) -> Dict[str, float]:
        """
        æå–ç¤¾äº¤åª’ä½“ç‰¹å®šç‰¹å¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            ç¤¾äº¤åª’ä½“ç‰¹å¾å­—å…¸
        """
        features = {}
        
        # ç¤¾äº¤åª’ä½“æ ‡è®°
        features['hashtag_count'] = len(re.findall(r'#\w+', text))
        features['mention_count'] = len(re.findall(r'@\w+', text))
        features['url_count'] = len(re.findall(r'http[s]?://\S+', text))
        features['emoticon_count'] = len(re.findall(r'[:;=]-?[)(/|\\pPoO]', text))
        
        # é‡å¤å­—ç¬¦å’Œå•è¯
        features['repeated_char_count'] = len(re.findall(r'(.)\1{2,}', text))
        features['repeated_word_count'] = len(re.findall(r'\b(\w+)(\s+\1){2,}\b', text))
        
        # æ„Ÿå¹å·å’Œé—®å·çš„ä½¿ç”¨
        features['exclamation_density'] = text.count('!') / len(text) if text else 0
        features['question_density'] = text.count('?') / len(text) if text else 0
        
        return features
    
    def extract_all_features(self, text: str) -> Dict[str, float]:
        """
        æå–æ‰€æœ‰ç‰¹å¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ‰€æœ‰ç‰¹å¾çš„å­—å…¸
        """
        features = {}
        
        try:
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            features.update(self.extract_linguistic_features(text))
            features.update(self.extract_depression_features(text))
            features.update(self.extract_emotion_features(text))
            features.update(self.extract_social_media_features(text))
            # æ·»åŠ æƒ…æ„Ÿè½¬æŠ˜ç‰¹å¾ï¼ˆæ–°å¢ï¼‰
            features.update(self.extract_turnaround_features(text))
        except Exception as e:
            logger.error(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾
            features = self._get_default_features()
        
        return features
    
    def _get_default_features(self) -> Dict[str, float]:
        """
        è·å–é»˜è®¤ç‰¹å¾ï¼ˆå½“ç‰¹å¾æå–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
        
        Returns:
            é»˜è®¤ç‰¹å¾å­—å…¸
        """
        # è·å–æ‰€æœ‰ç‰¹å¾åç§°
        sample_text = "This is a sample text for feature extraction."
        try:
            default_features = self.extract_linguistic_features(sample_text)
            default_features.update(self.extract_depression_features(sample_text))
            default_features.update(self.extract_emotion_features(sample_text))
            default_features.update(self.extract_social_media_features(sample_text))
            default_features.update(self.extract_turnaround_features(sample_text))
            # å°†æ‰€æœ‰å€¼è®¾ä¸º0
            return {k: 0.0 for k in default_features.keys()}
        except:
            # å¦‚æœè¿é»˜è®¤ç‰¹å¾éƒ½è·å–å¤±è´¥ï¼Œè¿”å›åŸºæœ¬ç‰¹å¾
            return {
                'text_length': 0.0, 'word_count': 0.0, 'char_count': 0.0,
                'avg_word_length': 0.0, 'sentence_count': 0.0, 'avg_sentence_length': 0.0,
                'unique_words': 0.0, 'lexical_diversity': 0.0, 'type_token_ratio': 0.0,
                'exclamation_count': 0.0, 'question_count': 0.0, 'ellipsis_count': 0.0,
                'caps_words_count': 0.0, 'uppercase_ratio': 0.0
            }
    
    def extract_batch_features(self, texts: List[str]) -> List[Dict[str, float]]:
        """
        æ‰¹é‡æå–ç‰¹å¾
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            ç‰¹å¾å­—å…¸åˆ—è¡¨
        """
        if not texts:
            return []
        
        # è·å–æ ‡å‡†ç‰¹å¾åç§°
        standard_features = self._get_default_features()
        feature_names = list(standard_features.keys())
        
        features_list = []
        for text in texts:
            try:
                features = self.extract_all_features(text)
                # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨
                for name in feature_names:
                    if name not in features:
                        features[name] = 0.0
                features_list.append(features)
            except Exception as e:
                logger.warning(f"ç‰¹å¾æå–å¤±è´¥: {e}")
                # è¿”å›é»˜è®¤ç‰¹å¾å­—å…¸
                features_list.append(standard_features.copy())
        
        return features_list
    
    def get_feature_names(self) -> List[str]:
        """
        è·å–æ‰€æœ‰ç‰¹å¾åç§°
        
        Returns:
            ç‰¹å¾åç§°åˆ—è¡¨
        """
        # ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬è·å–ç‰¹å¾åç§°
        sample_text = "This is a sample text for feature extraction."
        features = self.extract_all_features(sample_text)
        return list(features.keys())
    
    def features_to_array(self, features_list: List[Dict[str, float]]) -> np.ndarray:
        """
        å°†ç‰¹å¾å­—å…¸åˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„
        
        Args:
            features_list: ç‰¹å¾å­—å…¸åˆ—è¡¨
            
        Returns:
            numpyæ•°ç»„
        """
        if not features_list:
            return np.array([])
        
        # è·å–æ‰€æœ‰ç‰¹å¾åç§°
        feature_names = list(features_list[0].keys())
        
        # è½¬æ¢ä¸ºæ•°ç»„
        array = np.array([[features.get(name, 0.0) for name in feature_names] 
                         for features in features_list])
        
        return array
    
    def extract_turnaround_features(self, text: str) -> Dict[str, float]:
        """
        æå–æƒ…æ„Ÿè½¬æŠ˜ç‰¹å¾ï¼ˆæ–°å¢ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æƒ…æ„Ÿè½¬æŠ˜ç‰¹å¾å­—å…¸
        """
        features = {}
        text_lower = text.lower()
        
        # è½¬æŠ˜è¯è®¡æ•°
        turnaround_count = 0
        for word in self.turnaround_words:
            if word in text_lower:
                turnaround_count += 1
        features['turnaround_word_count'] = turnaround_count
        
        # è½¬æŠ˜æ¨¡å¼åŒ¹é…
        pattern_matches = 0
        for pattern in self.compiled_turnaround_patterns:
            if pattern.search(text_lower):
                pattern_matches += 1
        features['turnaround_pattern_count'] = pattern_matches
        
        # è½¬æŠ˜å¼ºåº¦ï¼ˆè½¬æŠ˜è¯æ•°é‡ / æ€»è¯æ•°ï¼‰
        total_words = len(text.split())
        if total_words > 0:
            features['turnaround_intensity'] = turnaround_count / total_words
        else:
            features['turnaround_intensity'] = 0.0
        
        # æƒ…æ„Ÿå˜åŒ–åˆ†æ
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        features['sentence_count'] = len([s for s in sentences if s.strip()])
        
        # åˆ†æç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿ
        negative_first = 0
        positive_first = 0
        negative_last = 0
        positive_last = 0
        
        if sentences:
            first_sentence = sentences[0].lower()
            last_sentence = sentences[-1].lower()
            
            # ç¬¬ä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿ
            for word in self.emotion_words['negative']:
                if word in first_sentence:
                    negative_first += 1
            for word in self.emotion_words['positive']:
                if word in first_sentence:
                    positive_first += 1
            
            # æœ€åä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿ
            for word in self.emotion_words['negative']:
                if word in last_sentence:
                    negative_last += 1
            for word in self.emotion_words['positive']:
                if word in last_sentence:
                    positive_last += 1
        
        features['negative_first_sentence'] = negative_first
        features['positive_first_sentence'] = positive_first
        features['negative_last_sentence'] = negative_last
        features['positive_last_sentence'] = positive_last
        
        # æƒ…æ„Ÿå˜åŒ–æ–¹å‘
        if negative_first > positive_first and positive_last > negative_last:
            features['sentiment_turnaround'] = 1.0  # è´Ÿé¢è½¬æ­£é¢
        elif positive_first > negative_first and negative_last > positive_last:
            features['sentiment_turnaround'] = -1.0  # æ­£é¢è½¬è´Ÿé¢
        else:
            features['sentiment_turnaround'] = 0.0  # æ— å˜åŒ–æˆ–åŒå‘
        
        # æ•´ä½“æƒ…æ„Ÿå€¾å‘ï¼ˆè€ƒè™‘è½¬æŠ˜ï¼‰
        negative_count = sum(1 for word in self.emotion_words['negative'] if word in text_lower)
        positive_count = sum(1 for word in self.emotion_words['positive'] if word in text_lower)
        
        if negative_count + positive_count > 0:
            base_sentiment = (positive_count - negative_count) / (negative_count + positive_count)
            
            # å¦‚æœæœ‰è½¬æŠ˜è¯ï¼Œè°ƒæ•´æƒ…æ„Ÿåˆ†æ•°
            if turnaround_count > 0:
                # è½¬æŠ˜è¯è¶Šå¤šï¼Œæƒ…æ„Ÿè¶Šåå‘ä¸­æ€§æˆ–æ­£é¢
                adjustment = min(turnaround_count * 0.2, 0.5)  # æœ€å¤šè°ƒæ•´0.5
                adjusted_sentiment = base_sentiment + adjustment
                features['overall_sentiment'] = max(-1.0, min(1.0, adjusted_sentiment))
            else:
                features['overall_sentiment'] = base_sentiment
        else:
            features['overall_sentiment'] = 0.0
        
        return features
