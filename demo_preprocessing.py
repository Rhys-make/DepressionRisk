#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†æ¨¡å—æ¼”ç¤ºè„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®é¢„å¤„ç†æ¨¡å—å¤„ç†ç¤¾äº¤åª’ä½“æ–‡æœ¬
"""

import pandas as pd
import numpy as np
from src.data_processing import TextCleaner, FeatureExtractor, DataProcessor

def demo_text_cleaner():
    """æ¼”ç¤ºæ–‡æœ¬æ¸…æ´—å™¨"""
    print("ğŸ§¹ æ–‡æœ¬æ¸…æ´—å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ–‡æœ¬æ¸…æ´—å™¨
    cleaner = TextCleaner()
    
    # ç¤ºä¾‹æ–‡æœ¬ï¼ˆæ¨¡æ‹Ÿç¤¾äº¤åª’ä½“æ–‡æœ¬ï¼‰
    sample_texts = [
        "ä»Šå¤©å¿ƒæƒ…çœŸçš„å¾ˆå·®:( æ„Ÿè§‰ä»€ä¹ˆéƒ½ä¸æƒ³åš... #depression #sad",
        "I'm feeling so down today :( nothing seems to matter anymore...",
        "ä»Šå¤©å¾ˆå¼€å¿ƒï¼:) å’Œæœ‹å‹ä¸€èµ·å‡ºå»ç©ï¼Œæ„Ÿè§‰è¶…æ£’çš„ï¼",
        "Feeling great today! :D Had an amazing time with friends!",
        "ä¸ºä»€ä¹ˆæˆ‘æ€»æ˜¯è¿™ä¹ˆæ²¡ç”¨... æ„Ÿè§‰è‡ªå·±æ˜¯ä¸ªå¤±è´¥è€… T_T",
        "Why am I always so useless... I feel like such a failure T_T"
    ]
    
    print("åŸå§‹æ–‡æœ¬:")
    for i, text in enumerate(sample_texts, 1):
        print(f"{i}. {text}")
    
    print("\næ¸…æ´—åçš„æ–‡æœ¬:")
    for i, text in enumerate(sample_texts, 1):
        cleaned = cleaner.clean_text(text)
        print(f"{i}. {cleaned}")
    
    print("\næ–‡æœ¬ç‰¹å¾:")
    for i, text in enumerate(sample_texts, 1):
        features = cleaner.extract_features(text)
        print(f"{i}. é•¿åº¦: {features['text_length']}, å•è¯æ•°: {features['word_count']}, è¡¨æƒ…ç¬¦å·: {features['emoticon_count']}")

def demo_feature_extractor():
    """æ¼”ç¤ºç‰¹å¾æå–å™¨"""
    print("\nğŸ” ç‰¹å¾æå–å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = FeatureExtractor()
    
    # ç¤ºä¾‹æ–‡æœ¬
    sample_texts = [
        "I feel so sad and depressed today. Nothing makes me happy anymore.",
        "I'm feeling great! Life is wonderful and I'm so excited about everything!",
        "Sometimes I think about ending it all. I'm so tired of this pain.",
        "I love spending time with friends and family. Everything is perfect!"
    ]
    
    print("ç¤ºä¾‹æ–‡æœ¬:")
    for i, text in enumerate(sample_texts, 1):
        print(f"{i}. {text}")
    
    print("\næå–çš„ç‰¹å¾:")
    for i, text in enumerate(sample_texts, 1):
        print(f"\næ–‡æœ¬ {i}:")
        features = extractor.extract_all_features(text)
        
        # æ˜¾ç¤ºå…³é”®ç‰¹å¾
        key_features = {
            'è¯­è¨€å­¦ç‰¹å¾': ['text_length', 'word_count', 'lexical_diversity'],
            'æŠ‘éƒç‰¹å¾': ['total_depression_count', 'depression_density'],
            'æƒ…æ„Ÿç‰¹å¾': ['emotion_polarity', 'emotion_intensity'],
            'ç¤¾äº¤åª’ä½“ç‰¹å¾': ['hashtag_count', 'mention_count']
        }
        
        for category, feature_names in key_features.items():
            print(f"  {category}:")
            for name in feature_names:
                if name in features:
                    print(f"    {name}: {features[name]:.4f}")

def demo_data_processor():
    """æ¼”ç¤ºæ•°æ®å¤„ç†å™¨"""
    print("\nğŸ›ï¸ æ•°æ®å¤„ç†å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    sample_data = pd.DataFrame({
        'text': [
            "I feel so sad and depressed today. Nothing makes me happy anymore.",
            "I'm feeling great! Life is wonderful and I'm so excited about everything!",
            "Sometimes I think about ending it all. I'm so tired of this pain.",
            "I love spending time with friends and family. Everything is perfect!",
            "I feel worthless and like a complete failure. What's the point?",
            "Today was amazing! I accomplished so much and feel proud of myself!"
        ],
        'label': [1, 0, 1, 0, 1, 0]  # 1=æŠ‘éƒé£é™©, 0=æ­£å¸¸
    })
    
    print("ç¤ºä¾‹æ•°æ®:")
    print(sample_data)
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    processor = DataProcessor(
        text_column='text',
        label_column='label',
        min_text_length=10,
        max_text_length=500
    )
    
    # å¤„ç†æ•°æ®
    print("\nå¤„ç†æ­¥éª¤:")
    print("1. å¤„ç†ç¤¾äº¤åª’ä½“æ•°æ®...")
    processed_data = processor.process_social_media_data(sample_data)
    print(f"   å¤„ç†åæ•°æ®é‡: {len(processed_data)} è¡Œ")
    
    print("2. å‡†å¤‡ç‰¹å¾...")
    features, labels = processor.prepare_features(processed_data, label_column='label')
    print(f"   ç‰¹å¾ç»´åº¦: {features.shape}")
    print(f"   æ ‡ç­¾æ•°é‡: {len(labels)}")
    
    print("3. åˆ†å‰²æ•°æ®...")
    train_data, test_data = processor.create_train_test_split(processed_data, test_size=0.3, stratify='label')
    print(f"   è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    
    print("4. ç‰¹å¾ç»Ÿè®¡:")
    if features is not None:
        print(f"   ç‰¹å¾æ•°é‡: {features.shape[1]}")
        print(f"   ç‰¹å¾èŒƒå›´: [{features.min():.4f}, {features.max():.4f}]")
        print(f"   ç‰¹å¾å‡å€¼: {features.mean():.4f}")
        print(f"   ç‰¹å¾æ ‡å‡†å·®: {features.std():.4f}")
    
    print("5. æ ‡ç­¾åˆ†å¸ƒ:")
    if labels is not None:
        unique_labels, counts = np.unique(labels, return_counts=True)
        for label, count in zip(unique_labels, counts):
            print(f"   æ ‡ç­¾ {label}: {count} æ ·æœ¬ ({count/len(labels)*100:.1f}%)")
    else:
        print("   æ ‡ç­¾åˆ—æœªæ‰¾åˆ°")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š æ•°æ®é¢„å¤„ç†æ¨¡å—å®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # æ¼”ç¤ºå„ä¸ªç»„ä»¶
        demo_text_cleaner()
        demo_feature_extractor()
        demo_data_processor()
        
        print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“ æ€»ç»“:")
        print("- æ–‡æœ¬æ¸…æ´—å™¨: æ¸…ç†å’Œæ ‡å‡†åŒ–ç¤¾äº¤åª’ä½“æ–‡æœ¬")
        print("- ç‰¹å¾æå–å™¨: æå–48ä¸ªå¤šç»´ç‰¹å¾")
        print("- æ•°æ®å¤„ç†å™¨: æ•´åˆå¤„ç†æµç¨‹ï¼Œå‡†å¤‡è®­ç»ƒæ•°æ®")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„ä¾èµ–åŒ…")

if __name__ == "__main__":
    main()
